[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Marc Palacín and I’m a Data Engineer, Software Engineer and hopefully a future DevOps engineer!\nI love tinkering with electronics and hardware, and process automation is my passion."
  },
  {
    "objectID": "devops-exercises/explore-california/00-introduction.html",
    "href": "devops-exercises/explore-california/00-introduction.html",
    "title": "Explore California",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "homelab-project/00-introduction.html",
    "href": "homelab-project/00-introduction.html",
    "title": "Homelab project introduction",
    "section": "",
    "text": "First of all\nBig thank you to Lars Kiesow and his awesome guide on how to configure a home lab with Proxmox VE from the start and very well explained, no shortcuts at all.\n\nLink to the guide here\nYou can also check the original MD files in its GitHub repository here\nAlso check the proxmox branch for some new insights still not pushed to the main branch here\n\n\n\nRationale\n\n\n\n\n\n\nTODO\n\n\n\nPending. Add some architecture diagram to better understand what we’re trying to achieve."
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#main-concepts",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#main-concepts",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Main concepts",
    "text": "Main concepts\nKubernetes components: A Kubernetes cluster is mainly composed of the following components:\n\nControl Plane components: these make global decisions about the cluster (scheduling pods) as well as detecting and responding to cluster events\n\nThey can be run on any machine in the cluster; however, for simplicity, set up scripts typically start all control plane components on the same machine and do not run user containers there\nSome of the common control plane components are: kube-apiserver, etcd, kube-scheduler, kube-control-manager and cloud-control-manager\n\nNode components: these run on every node, maintaining running pods and providing the Kubernetes runtime environment\n\nSome of the common node components are: kubelet, kube-proxy and container runtime\n\nAddons: they use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features\n\nBecause these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace\nSelected addons are DNS, Web UI (Dashboard), Container Resource Monitoring and Cluster-level Logging\n\n\nAs described above, usually all control plane components are deployed into the same machine or node. Another usual thing to do is to isolate the control plane nodes to prevent them to act as workers; this ensures that their critical mission of overseeing the whole cluster isn’t compromised due to a highly demanding user container.\nWhen dealing with High-Availability, we’ll configure more than one control plane node to ensure redundancy."
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-configuration",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-configuration",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Ansible configuration",
    "text": "Ansible configuration\nFollow the guide’s instructions, clone its git repository and make sure you update the following files. Make a copy of the /inventory/sample directory and name it /inventory/my-cluster. After that, modify the following files according to your servers:\n\n\ninventory/my-cluster/hosts.ini\n\n[master]\nk3s-control-1\nk3s-control-2\nk3s-control-3\n\n[node]\nk3s-worker-1\nk3s-worker-2\n\n\n\n\n\n\n\nSSH keys\n\n\n\nRemember to exchange your SSH public key with the rest of the servers, else Ansible will fail to launch the commands.\n\n\n\n\ninventory/my-cluster/group_vars/all.yml\n\n# this is the user that has ssh access to these machines\n# You have to exchange your public SSH key with them\nansible_user: \"common-user-to-all-servers\"\n\n# k3s_token is required  masters can talk together securely\n# this token should be alpha numeric only\nk3s_token: \"some-SUPER-DEDEUPER-secret-password\"\n\n# metallb ip range for load balancer\n# You might want to avoid matching ranges with your network     \nmetal_lb_ip_range: \"192.168.30.80-192.168.30.90\"\n\n\n\n\n\n\n\nUsing Alpine server for Ansible\n\n\n\nYou may need to launch the following commands:\n# For cluster servers in different subnets\napk add py3-netaddr\n\n# For installing ansible-galaxy requirements\n# Regenerate SSL certificates\n# Guide: https://wiki.alpinelinux.org/wiki/Generating_SSL_certs_with_ACF\n/sbin/setup-acf\napk add acf-openssl"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-playbook-launch",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-playbook-launch",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Ansible playbook launch",
    "text": "Ansible playbook launch\nNow launch the following command from the repository root:\n\n\nAnsible command\n\n# Install ansible requirements\nansible-galaxy install -r ./collections/requirements.yml\n\n# Launch the installation playbook\nansible-playbook ./site.yml -i ./inventory/my-cluster/hosts.ini\n\nCheck for any errors during the ansible-playbook execution. If everything is OK, proceed to copy the k3s cluster config into the central server:\n\n\nCopying cluster config and testing\n\n# Make sure the '.kube' folder exists in the local server\nmkdir ~/.kube\n\n# Copy the config from one of the master nodes\nscp common-user-to-all-servers@k3s-control-1:~/.kube/config ~/.kube/config\n\n# Check that we can access the cluster\nsudo kubectl get nodes\n\n\n\n\n\n\n\nWorker nodes not showing in the cluster\n\n\n\nCheck what’s wrong with the ansible steps when reaching workers"
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html",
    "href": "homelab-project/requirements/00-requirements.html",
    "title": "Homelab Requirements",
    "section": "",
    "text": "Any relatively modern desktop computes can be used as a homelab. In my case I’ll be using an HP Compaq Elite 8300 with an Intel Core i5 (4 cores), 24Gb or RAM and a single NIC."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#virtualization-suite",
    "href": "homelab-project/requirements/00-requirements.html#virtualization-suite",
    "title": "Homelab Requirements",
    "section": "Virtualization suite",
    "text": "Virtualization suite\nIn order to comply with the previous statement, we’ll be using Proxmox VE, a Virtualization suite that acts as a HyperVisor as well as a Linux Containers deployer."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#network-management",
    "href": "homelab-project/requirements/00-requirements.html#network-management",
    "title": "Homelab Requirements",
    "section": "Network management",
    "text": "Network management\nVirtual hosts should have their own internal network so they don’t interfere with other local DHCP servers or even access external networks at all."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#self-hosted-software",
    "href": "homelab-project/requirements/00-requirements.html#self-hosted-software",
    "title": "Homelab Requirements",
    "section": "Self-hosted software",
    "text": "Self-hosted software\nWe’d like to host our own services, and again the best approach should be to deploy containers, either as-is or in an orchestrated environment such as Kubernetes."
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "",
    "text": "Docker"
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#docker-and-docker-compose",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#docker-and-docker-compose",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "Docker and Docker compose",
    "text": "Docker and Docker compose\n\nI’m going to follow Technotim’s guide as my server OS is Ubuntu.\nThere’s really nothing much to say here, just follow the instructions, make sure that you have Internet access to install the packages and check that your user can execute docker commands, like docker version, and docker compose.\nAlso, remember to add your user to the docker group so you don’t need privileged permissions to operate with it:\nsudo usermod -aG docker $USER\nOnce done, log out and log in again to your server to apply the changes on the user groups."
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#overlay-network",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#overlay-network",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "Overlay network",
    "text": "Overlay network\nAn overlay network is a type of virtual network in Docker that allows multiple Docker hosts to communicate between them. This is pretty handy if, for example, you need some services deployed with Docker in a host to access an authentication service that is hosted on a different Docker host.\n\n\n\n\n\n\nAttention!\n\n\n\nIn order to use this, do we need to install docker swarm?\n\n\nYou can find the official Docker docs regarding overlay networking here."
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#get-a-domain-name",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#get-a-domain-name",
    "title": "01 - External access - Cloudflare",
    "section": "Get a Domain name",
    "text": "Get a Domain name"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#create-a-cloudflare-account",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#create-a-cloudflare-account",
    "title": "01 - External access - Cloudflare",
    "section": "Create a Cloudflare account",
    "text": "Create a Cloudflare account"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#setup-the-domain-name-in-cloudflare",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#setup-the-domain-name-in-cloudflare",
    "title": "01 - External access - Cloudflare",
    "section": "Setup the Domain name in Cloudflare",
    "text": "Setup the Domain name in Cloudflare"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#zero-trust-dashboard",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#zero-trust-dashboard",
    "title": "01 - External access - Cloudflare",
    "section": "Zero Trust Dashboard",
    "text": "Zero Trust Dashboard\n\n\n\n\n\n\nTODO\n\n\n\nAdd some screenshots of the nameserver creation procedure in Zero Trust Dashboard"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#deploying-cloudflared-as-docker-container",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#deploying-cloudflared-as-docker-container",
    "title": "01 - External access - Cloudflare",
    "section": "Deploying cloudflared as docker container",
    "text": "Deploying cloudflared as docker container\nAdapt the proposed docker compose file in this link to launch it as a standalone container.\nCloudflare recommends to create only a tunnel for each network, so we’ll deploy it on a container within our private network:\n\n\ndocker-compose.yml\n\nversion: '3.2'\nname: cloudflared\nservices:\n  tunnel:\n    #container_name: cloudflared-tunnel\n    image: cloudflare/cloudflared\n    # This sysctl param change doesn't seem to work on Apache OSes\n    sysctls:\n      net.core.rmem_max: 2500000\n    restart: unless-stopped\n    command: tunnel --metrics 0.0.0.0:3333 run\n    environment:\n      # Add your cloudflare token inside a secured '.env' file\n      - TUNNEL_TOKEN=${CLOUDFLARE_TOKEN}\n    # Add autoheal feature to ensure it's restarted on failure\n    labels:\n      - autoheal=true\n    # TODO: Official cloudflared image doesn't have neither curl nor wget nor dig\n    # so we can't launch the healthcheck! We either create a new image that installs\n    # any of the required commands or try to get the health status from another container\n    # or from outside, exposing the port\n    #healthcheck:\n    #  test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:3333/ready\"]\n    #  interval: 10s\n    #  timeout: 3s\n    #  retries: 3\n    #  start_period: 30s\n\n  # Autoheal is a workaround to restart any container which healthcheck fails\n  #autoheal:\n  #  image: willfarrell/autoheal:1.2.0\n  #  volumes:\n  #    - \"/var/run/docker.sock:/var/run/docker.sock\"\n  #  environment:\n  #    AUTOHEAL_CONTAINER_LABEL: autoheal\n\nAdd the CLOUDFLARE_TOKEN value inside a secure .env file in the same dir as the docker-compose.yml file:\nCLOUDFLARE_TOKEN=token-provided-by-cloudflare\n\n\n\n\n\n\nWarning with system limits!\n\n\n\nIf you see a message like this one when launching docker compose logs tunnel:\nfailed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB)\nCheck the following link for a deep explanation: LINK\nWhat we must do is create an init container and change some system params, similar to what we are doing with SonarQube.\nAlso, we could try configuring the sysctl parameters using the docker compose file.\nOn some OS it doesn’t seem to work, such as Alpine, but if Cloudflare shows that the tunnel is connected, then this shouldn’t be a problem."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#prepare-an-entry-point-vm",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#prepare-an-entry-point-vm",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Prepare an entry-point VM",
    "text": "Prepare an entry-point VM\nWe’ll need an entry-point server that acts as both HTTP requests router and SSL provider (Traefik)."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#make-sure-yo-have-a-domain-name",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#make-sure-yo-have-a-domain-name",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Make sure yo have a Domain name",
    "text": "Make sure yo have a Domain name\n\n\n\n\n\n\nUse Cloudflare as DNS provider\n\n\n\nThis will ease the process and secure connections even more"
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-traefik",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-traefik",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Install Traefik",
    "text": "Install Traefik\n\n\n\n\n\n\nI couldn’t make it work!\n\n\n\nI don’t know if it’s because I’m using a Cloudflare Zero-Trust tunnel instead of assigning DNS records pointing towards my public IP, but either Traefik keeps responding with a 404 Page not found error or it fails due to too many redirections.\nI’ll try it again following the official documentation.\nThere’s another guide here that could be useful."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-portainer",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-portainer",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Install Portainer",
    "text": "Install Portainer\n\nPortainer is a tool that eases the management and deployment of Docker containers in your infrastructure. Its installation is also pretty straightforward:\n\nFor management purposes, we are going to create a docker folder in our home directory. This is where we’ll store all of our Docker services configurations:\n\nmkdir docker\necho $PWD\n# This should return the following\n\n\n\n\n\n\nTODO\n\n\n\nPending to install Portainer and control all Docker hosts"
  },
  {
    "objectID": "homelab-project/self-hosted-software/03-dashy.html",
    "href": "homelab-project/self-hosted-software/03-dashy.html",
    "title": "03 - Homelab dashboard - Dashy",
    "section": "",
    "text": "Dashy is a lightweight dashboard that allows users, devops and sysadmins to quickly access to its services:\n\nGithub repository\nDocumentation"
  },
  {
    "objectID": "homelab-project/self-hosted-software/03-dashy.html#installation",
    "href": "homelab-project/self-hosted-software/03-dashy.html#installation",
    "title": "03 - Homelab dashboard - Dashy",
    "section": "Installation",
    "text": "Installation\nFollow this video for a simple Docker deployment and initial configuration:\n\n\nSynology\nThe docker image can also be deployed inside a Synology NAS. Follow the official Dashy’s documentation on how to do it.\n\n\nDocker compose\nWe can also create a docker-compose.yml file for an easier deployment. Follow Dashy’s documentation and modify it according to your own architecture.\nJust add some volumes for both config.yml and the item-icons folder so we don’t lose them when restarting the container.\n\n\ndocker-compose.yml\n\n---\nversion: \"3.8\"\nservices:\n  dashy:\n    # To build from source, replace 'image: lissy93/dashy' with 'build: .'\n    # build: .\n    image: lissy93/dashy\n    container_name: Dashy\n    # Pass in your config file below, by specifying the path on your host machine\n    # volumes:\n      # - /root/my-config.yml:/app/public/conf.yml\n    ports:\n      - 4000:80\n    # Set any environmental variables\n    environment:\n      - NODE_ENV=production\n    # Specify your user ID and group ID. You can find this by running `id -u` and `id -g`\n    #  - UID=1000\n    #  - GID=1000\n    # Specify restart policy\n    restart: unless-stopped\n    # Volumes\n    volumes:\n      - $PWD/public/conf.yml:/app/public/conf.yml\n      - $PWD/public/item-icons:/app/public/item-icons\n    # Configure healthchecks\n    healthcheck:\n      test: ['CMD', 'node', '/app/services/healthcheck']\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n\nThen locally clone the GitHub repository of icons to beautify the panels:\ncd public/item-icons\ngit clone https://github.com/walkxcode/dashboard-icons.git\nAnd here is a basic config.yml with enabled built-in authentication and some services already configured into its proper sections:\n\n\n\n\n\n\nRemember to change the admin hash\n\n\n\nThe hash parameter within auth.users[admin].hash is the password converted to a sha256 hash. You can obtain it by simply invoking the following command from a terminal:\necho -n \"your-admin-super-secret-password\" | sha256sum\nThe hash is the first 64 characters of the output (ignore the empty spaces and dashes at the end).\n\n\n\n\npublic/conf.yml\n\nappConfig:\n  theme: colorful\n  layout: auto\n  iconSize: large\n  language: en\n  auth:\n    enableGuestAccess: true\n    users:\n      - user: admin\n        hash: hash-of-a-password-you-choose-using-sha256-hashing\n        type: admin\ndefaultOpeningMethod: newtab\nwebSearch:\n    disableWebSearch: false\n    searchEngine: duckduckgo\n    openingMethod: newtab\n    searchBangs: {}\n  enableFontAwesome: true\n  enableMaterialDesignIcons: true\n  allowConfigEdit: true\npageInfo:\n  title: Home Lab\n  description: Welcome to your Home Lab!\n  navLinks:\n    - title: GitHub\n      path: https://github.com/ProtossGP32/DevOps-training-projects\n    - title: Introduction to HomeLab\n      path: https://protossgp32.github.io/DevOps-training-projects/homelab-project/00-introduction.html\n  footerText: ''\nsections:\n  - name: Server administration\n    icon: fas fa-server\n    items:\n      - title: Proxmox\n        description: Virtualisation manager\n        url: https://pve.protossnet.local\n        icon: dashboard-icons/png/proxmox.png\n        statusCheck: true\n      - title: phpLDAPadmin\n        description: LDAP administrator\n        url: http://ldap-server.protossnet.local:8080\n        icon: dashboard-icons/png/phpldapadmin.png\n        statusCheck: true\n    displayData:\n      sortBy: default\n      rows: 1\n      cols: 1\n      collapsed: false\n      hideForGuests: true\n\n  - name: Productivity Tools\n    icon: fas fa-regular fa-window\n    items:\n      - title: NextCloud\n        description: Cloud office suite and collaboration platform\n        url: http://sonic.protossnet.local:8080\n        icon: dashboard-icons/png/nextcloud.png\n        statusCheck: true\n      - title: OpenProject\n        description: Projects management suite\n        url: http://sonic.protossnet.local:8081\n        icon: dashboard-icons/png/openproject.png\n        statusCheck: true\n      - title: SonarQube\n        description: Static code analysis suite\n        url: http://sonic.protossnet.local:9000\n        icon: dashboard-icons/png/sonarqube.png\n      - title: VS Code Web\n        description: Cloud based VS Code development environment\n        icon: dashboard-icons/png/vs-code.png\n        statusCheck: true\n\n  - name: User management\n    icon: fas fa-light fa-user\n    displayData:\n      sortBy: default\n      rows: 1\n      cols: 1\n      collapsed: false\n      hideForGuests: false\n    items:\n      - title: Self Service Password\n        description: Allows to change your user password\n        icon: dashboard-icons/png/ltb-logo.png\n        url: https://ssp-protoss.cifoweb.dev/\n        statusCheck: true"
  },
  {
    "objectID": "homelab-project/self-hosted-software/04-openldap.html",
    "href": "homelab-project/self-hosted-software/04-openldap.html",
    "title": "04 - Authentication - OpenLDAP",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy SonarQube, both on internal network and accessible from outside with HTTPS partially enabled\n\n\n\n\nAn open-source LDAP server\n\nWeb page: Link\nSource code: GitLab and GitHub mirror\nDocker image: Docker Hub - Non-privileged image\n\n\n\n\nAn open-source LDAP administration service with a web interface\n\nWeb page: Link\nSource code: GitHub and the repository which the Docker image that we use is based\nDocker image: Docker Hub\n\n\n\n\nAn open-source web interface that allows LDAP users to change their own passwords.\n\nWeb page: Link\nSource code: GitHub\nDocker image: Docker Hub\n\nThe docker container needs a config file named config.inc.local.php that overwrites the default values defined in config.inc.php; this file is provided in the docker-compose.yml file as a volume.\nThere are a lot of parameters that can be configured, but for the sake of simplicity only LDAP server parameters and password policies are defined here:\n\n\nconfig.inc.local.php\n\n<?php\n// A random keyphrase is required to initialize the service\n$keyphrase = getenv('SSP_KEYPHRASE');\n$debug = false;\n\n// LDAP server config\n$ldap_url = \"ldap://openldap:1389\";\n$ldap_starttls = false;\n$ldap_binddn = \"cn=admin,dc=protossnet,dc=local\";\n$ldap_bindpw = getenv('LDAP_BIND_PASSWORD');\n\n$ldap_base = \"ou=users,dc=protossnet,dc=local\";\n$ldap_login_attribute = \"uid\";\n$ldap_fullname_attribute = \"displayName\";\n$ldap_filter = \"(&(objectClass=inetOrgPerson)($ldap_login_attribute={login}))\";\n\n// Password policies that new passwords must comply\n$pwd_min_length = 8;\n$pwd_max_length = 16;\n$pwd_min_lower = 1;\n$pwd_min_upper = 1;\n$pwd_min_digit = 1;\n$pwd_min_special = 1;\n$pwd_special_chars = \"^@%!-_\";\n$pwd_no_reuse = true;\n$pwd_diff_login = true;\n$pwd_diff_last_min_chars = 0;\n$pwd_no_special_at_ends = false;\n\n// \"manager\" means that the user defined in ldap_binddn is the responsible of\n// changing the user's passwords within LDAP (the rest of users don't have enough privileges)\n$who_change_password = \"manager\";\n$lang = \"en\";\n$allowed_lang = array(\"en\");\n?>"
  },
  {
    "objectID": "homelab-project/self-hosted-software/05-nextcloud.html",
    "href": "homelab-project/self-hosted-software/05-nextcloud.html",
    "title": "05 - Productivity tools - NextCloud",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy NextCloud, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/self-hosted-software/06-openproject.html",
    "href": "homelab-project/self-hosted-software/06-openproject.html",
    "title": "06 - Productivity tools - OpenProject",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy OpenProject, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/self-hosted-software/07-sonarqube.html",
    "href": "homelab-project/self-hosted-software/07-sonarqube.html",
    "title": "07 - Code analysis - SonarQube",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy SonarQube, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "",
    "text": "TODO\n\n\n\nPending to describe Proxmox"
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html#ip-address-assigned-via-dhcp",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html#ip-address-assigned-via-dhcp",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "IP address assigned via DHCP",
    "text": "IP address assigned via DHCP\nThis way the DHCP server will assign the correct domain name and the server will be accessible by its domain name.\nA server should always have a predictable IP, so configure the DHCP server to lease the same IP to the Proxmox server network MAC address.\n\n\n\n\n\n\nTODO\n\n\n\nPending to elaborate\n\n\n\n\n\n\n\n\nRegarding DNS and DHCP servers…\n\n\n\nLater on, we’ll configure our own virtual DHCP and DNS servers to provide internal IPs to our VM and containers."
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html#allow-https-on-port-443-via-nginx",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html#allow-https-on-port-443-via-nginx",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "Allow HTTPS on Port 443 via Nginx",
    "text": "Allow HTTPS on Port 443 via Nginx\nAs Lars mentions in his guide, the web interface of Proxmox is available on port 8006 instead of the default HTTPS port 443, and HTTP on Port 80 is not available at all.\nHaving HTTPS acesses through ports different than 443 can be very bothersome as we need to remember what exact port is the one used on each service.\nIn order to solve this, we’ll use a light-weight web server that also acts as a reverse proxy natively in the Proxmox server, so we can redirect 443 requests to the 8006 port.\n\n\n\n\n\n\nAbout Nginx…\n\n\n\nLater on we’ll be also deploying other Nginx servers within our internal networks to redirect traffic to each one of our services\n\n\n\nInstall and Configure Nginx\nConnect via SSH to the PVE server as root:\nssh root@<proxmox-ip>\n# or\nssh root@<proxmox-hostname>.<local-domain>\nInstall the nginx-light package:\napt-get install nginx-light\nA web server should already be up and running. Verify it by accessing http://<proxmox-ip>/ or http://<proxmox-hostname>.<local-domain>, a blank welcome screen should appear.\n\n\n\nNginx welcome web page\n\n\nOnce done, remove the default Nginx configuration from the Proxmox server…\nrm /etc/nginx/sites-*/default\n…and create a new file /etc/nginx/sites-available/proxmox-web-interface with the following content:\n\n\n/etc/nginx/sites-available/proxmox-web-interface\n\nserver {\n  # Enforce HTTPS by redirecting requests\n  listen 80;\n  listen [::]:80;\n  server_name pve.protossnet.local;\n\n  location / {\n    return 301 https://pve.protossnet.local$request_uri;\n  }\n}\n\nserver {\n  listen 443 ssl http2;\n  listen [::]:443 ssl http2;\n  server_name pve.protossnet.local;\n\n  ssl_certificate_key /etc/pve/local/pve-ssl.key;\n  ssl_certificate     /etc/pve/local/pve-ssl.pem;\n\n  # Proxy configuration\n  location / {\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_pass https://127.0.0.1:8006;\n    proxy_buffering off;\n    client_max_body_size 0;\n    proxy_connect_timeout  3600s;\n    proxy_read_timeout  3600s;\n    proxy_send_timeout  3600s;\n    send_timeout  3600s;\n    proxy_set_header Host $host;\n    proxy_ssl_name $host;\n    proxy_set_header X-Forwarded-For $remote_addr;\n  }\n}\n\n\n\n\n\n\n\nNote\n\n\n\nLars has additional and more complex Nginx configs for the web interface, it’s worth checking them out to learn about how Nginx works and how powerful it can be!\n\n\nAfter updating the configuration, we must create a softlink from the sites-enabled directory to make it available:\ncd /etc/nginx/sites-enabled\nln -s /etc/nginx/sites-available/proxmox-web-interface\nFinally, we check if Nginx can spot any errors:\nnginx -t\n\n\nRestart Nginx and Enable it by Default\nTo start Nginx and make sure it starts automatically after a system reboot, run:\n# Restart the Nginx service\nsystemctl restart nginx.service\n# Permanently enable the Nginx service\nsystemctl enable nginx.service\nFinally, make sure that Nginx will start only after Proxmox starts, since the certificates may otherwise not be available yet. To do that, create a file /etc/systemd/system/nginx.service.d/override.conf with the content:\n[Unit]\nRequires=pve-cluster.service\nAfter=pve-cluster.service\nAlternatively, you can also use systemctl edit nginx.service to edit this file.\nWith the service restarted, check that we can access the Proxmox VE web interface through the expected HTTPS port by going to https://<proxmox-hostname>.<local-domain> or https://<proxmox-ip>:\n\n\n\nProxmox - Nginx correctly redirecting HTTPS request\n\n\n\n\n\n\n\n\nWhat about the insecure HTTPS warning?\n\n\n\nRight now Nginx is using the SSL certificates provided by Proxmox on installation time. We should provide valid SSL certifications to avoid this, such as the ones that Let’s Encript can generate.\n\n\n\n\nMake Proxmox UI Service Listen to Localhost Only\n\n\n\n\n\n\nLars suggests this change to ensure that everyone is using the Nginx set-up, but I don’t really understand what exactly means. I’ve applied the change and at least I can still access it from within my local network.\n\n\n\nCreate /etc/default/pveproxy and set:\nLISTEN_IP=\"127.0.0.1\"\nThen restart the pveproxy service:\nsystemctl restart pveproxy.service"
  },
  {
    "objectID": "homelab-project/virtualization-server/02-virtual-networks.html",
    "href": "homelab-project/virtualization-server/02-virtual-networks.html",
    "title": "Part 2 - Internal Virtual Networks",
    "section": "",
    "text": "Virtual networks are network interfaces that act as fake NICs and allow multiple VM or LXC to communicate within the same subnet. A virtual network can be bridged or bonded to a physical NIC or can live on its own without access to external devices (we’ll call them internal networks); the latter are useful when we have a cluster of resources that don’t need to reach external networks."
  },
  {
    "objectID": "homelab-project/virtualization-server/02-virtual-networks.html#build-a-proxmox-internal-network",
    "href": "homelab-project/virtualization-server/02-virtual-networks.html#build-a-proxmox-internal-network",
    "title": "Part 2 - Internal Virtual Networks",
    "section": "Build a Proxmox Internal Network",
    "text": "Build a Proxmox Internal Network\nFirst of all, we need to create a new network interface on our Proxmox server and assign it a network. This network bridge can then later be used to put machines on the internal network.\nThis gives us a fully functional internal network to use.\n\n\n\n\n\n\nMind the network transparency!\n\n\n\nAs Lars explains, the downside of having internal networks for clusters of VM is that we lose network transparency in that if a VM starts to behave erratically, its effect on the network level will be masked behind the Proxmox real IP, thus being unable to quickly identify the actual culprit.\nOn production environments, this would end in the IT department cutting down network access to the Proxmox server, and therefore to any VM machine inside it.\n\n\nAs a starting point, create a new network bridge in the Proxmox web interface:\n\nHead to Datacenter → Proxmox server name → Network, clic Create → Linux Bridge and set something like:\n\nName: vmbr1\nIPv4/CIDR: 10.0.0.1/16\nAutostart: ✓\nComment: Internal network\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis example provides an internal /16 network, more than enough for our inter machine configuration (65.536 IP addresses, being 65.534 usable!):\n\nNetwork Address: 10.0.0.0\nUsable IP range: 10.0.0.1 ~ 10.0.255.254\nBroadcast Address: 10.0.255.255\n\nIn most cases, using a /24 should be enough (253 IP addresses).\n\n\nThe file /etc/network/interfaces keeps the network interfaces configuration of the server. Once created the new interface, you should see the following new configuration block:\n\n\n/etc/network/interfaces\n\nauto vmbr1\niface vmbr1 inet static\n   address 10.0.0.1/16\n   bridge-ports none\n   bridge-stp off\n   bridge-fd 0\n\nOnce done, bring the new network interface up using:\nifup vmbr1\nAnd check its status by running:\nip a\n...\n4: vmbr1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether 32:7b:7f:8b:f8:9c brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.1/16 scope global vmbr1\n       valid_lft forever preferred_lft forever\nWith this, we have a fully functional network interface for our internal VMs and containers, but without access to the outside world.\n\n\n\n\n\n\nWhat about the state DOWN?\n\n\n\nThat’s because there’s no host using it yet. What’s important is that the inet is correctly configured as 10.0.0.1/16.\nOnce we attach this interface to some containers or VM, its state will change to UP.\n\n\n\n\n\n\n\n\nCommenting on Networks\n\n\n\nLars has a point when mentioning the importance of giving proper comments to each network interface. While it might seem unnecessary when dealing with just a few interfaces, we can forget about it once the number of appliances and services grow.\nThus, the default vmbr0 created on Proxmox installation time and with proper access to the outside shall be named External network, while the new vmbr1 shall be called Internal network. This way we avoid any kind of doubt when picking one of them. Use similar criteria with any new network interface.\n\n\n\nNAT Configuration\nOur current network interface vmbr1 can be useful for appliances that don’t require to access the internet (i.e. servers that simply execute local tasks and talk to each other). If we want this network interface to be able to reach the outside world, we need a NAT configuration.\n\n\n\n\n\n\nNAT\n\n\n\nNAT is the process where all traffic from a network is routed through a single IP address outside that network, and then return results to the request source.\n\n\nIn order to configure NAT in our interface, edit the file /etc/network/interface and add the following lines to the vmbr1 config block:\n\n\n/etc/network/interface\n\niface vmbr1 inet static\n   ...\n   post-up   echo 1 > /proc/sys/net/ipv4/ip_forward\n   post-up   iptables -t nat -A POSTROUTING -s '10.0.0.0/16' -o vmbr0 -j MASQUERADE\n   post-down iptables -t nat -D POSTROUTING -s '10.0.0.0/16' -o vmbr0 -j MASQUERADE\n\nWhat we are doing here is add rules to the interface, telling it how to route the packages in that network. This rule will create a dynamic source NAT where every packet from 10.0.0.0/16 will be sent to the interface vmbr0 (the External network) and the source IP address of this packet will be replaced by the primary IP of vmbr0 (the Proxmox server IP).\n\nSetting 1 to /proc/sys/net/ipv4/ip_forward enables to forward packages to another interface (TODO: needs references)\nThe -t nat -A POSTROUTING rule means that it allows the vmbr0 interface to route packages from the source -s '10.0.0.0/16', only after the interface vmbr1 is UP, that’s why it is defined as a post-up rule\nThe -t nat -D POSTROUTING rule does the opposite, denies any routing of packages from source -s '10.0.0.0' once the vmbr1 interface is DOWN, ensuring that no unexpected packages are routed through vmbr0\n\n\n\nTesting your Configuration\nFirst, make sure that both interfaces are up, either by executing ifup <interface name> or by rebooting the whole server.\nNext setup a simple container or virtual machine, assign vmbr1 interface as its Network interface and statically configure its IP and gateway according to the interface range. For example:\n\nBridge: vmbr1\nIPv4/CIDR: 10.0.0.2/16\nGateway (IPv4): 10.0.0.1\n\nOnce started, ssh into that server/container and try pinging any external server IP. Remember that there’s no DNS server for this network, so we can’t use domain names yet:\n\n\nPing test:\n\n# Let's try pinging Google\nping -4 -c 1 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=119 time=18.8 ms\n\n--- 8.8.8.8 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 18.764/18.764/18.764/0.000 ms\n\nThe ping command has returned results, so the network has access to external servers!"
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "",
    "text": "Wait, more DHCP servers? Why?\n\n\n\nYou might be thinking about the DHCP server that provides the IP address to the Proxmox server and wondering why this server can’t handle the IP assignment of the future VMs. Here’s why:\nA DHCP server only serves IPs within a certain established range, and we might not have access to it (restricted MAC access, limited number of IPs, etc…). Furthermore, we’ve created an internal network interface whose IP range shouldn’t match with the upstream server and is also masquerading its IPs behind the Proxmox server one (remember NAT?). Having said that, the next course of action is to create custom DHCP and DNS servers for each internal network.\nLater on, we’ll discuss how to allow multiple internal networks to talk to each other, but for now we’ll only use one.\nWe’ll be using dnsmasq to host a DHCP and DNS server either on the Proxmox server itself or on a machine or container running in Proxmox. What approach is better? It always depends on your needs:\nIn order to touch the least the Proxmox internal configurations and avoid messing with other active DHCP servers, we’ll go with the Container route. If you want to go with the Proxmox Server set-up, Lars also explains it here."
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#using-a-container",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#using-a-container",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "Using a container",
    "text": "Using a container\n\n\n\n\n\n\nContainer creation guide\n\n\n\nLXC - Linux Containers on Proxmox\n\n\nCreate a Debian container with 128 MB of RAM and assign it a static IPv4 address on the internal network (vmbr1 or check the interface comments). Make sure the IPv4 address is unique and does not clash with already assigned addresses. As DNS and DHCP services are quite important in a network, it’s usual to assign them the first or the last usable IP within the range; as we’re using 10.0.0.1 as our gateway (the Proxmox server), the next available one is 10.0.0.2:\n\nBridge: vmbr1\nIPv4: Static\nIPv4/CIDR: 10.0.0.2/16\nGateway (IPv4): 10.0.0.1\n\n\n\n\n\n\n\nAlpine instead of Debian\n\n\n\nFor the sake of using even more minimal distributions, I’ve used Alpine as it is very lightweigh. When using it, just remember that its package manager is apk instead of apt, and that installing new packages is done with add instead of install.\n\n\nMake sure to update the container as usual:\n# Alpine\napk update\napk upgrade\n\n# Debian\napt update\napt upgrade\nDebian only! The Proxmox Debian container template comes with systemd-resolved enabled. This service conflicts with dnsmasq and we don’t actually need it, so we disable it:\nsystemctl stop systemd-resolved.service\nsystemctl disable systemd-resolved.service\nNow we install dnsmasq:\n# Alpine\napk add dnsmasq\n\n# Debian\napt install dnsmasq\nAs usual, Debian will automatically start the service after its installation (this doesn’t happen in Alpine). But by default dnsmasq is configured as a DNS server only, so we need to add additional configuration to enable the DHCP feature. In any case, create a file /etc/dnsmasq.d/internal.conf with the following configuration:\n\n\n/etc/dnsmasq.d/internal.conf\n\n# Tells dnsmasq to never forward A or AAAA queries for plain names,\n# without dots or domain parts, to upstream nameservers.\n# If the name is not known from /etc/hosts or DHCP then a \"not found\" answer is\n# returned.\ndomain-needed\n\n# All reverse lookups for private IP ranges (ie 192.168.x.x, etc) which are not\n# found in /etc/hosts or the DHCP leases file are answered with \"no such domain\"\n# rather than being forwarded upstream.\nbogus-priv\n\n# Don't read /etc/resolv.conf. Get upstream servers only from the command line\n# or the dnsmasq configuration file.\nno-resolv\n\n# Later  versions of windows make periodic DNS requests which don't get sensible\n# answers from the public DNS and can cause problems by triggering dial-on-\n# demand links. This flag turns on an option to filter such requests. The\n# requests blocked are for records of types SOA and SRV, and type  ANY  where\n# the requested name has underscores, to catch LDAP requests.\nfilterwin2k\n\n# Add the domain to simple names (without a period) in /etc/hosts in the same\n# way as for DHCP-derived names. Note that this does not apply to domain names\n# in cnames, PTR records, TXT records etc.\nexpand-hosts\n\n# Specifies DNS domains for the DHCP server. This  has  two  effects;\n# firstly it causes the DHCP server to return the domain to any hosts which\n# request it, and secondly it sets the domain which is legal for DHCP-configured\n# hosts to claim.  In addition, when a suffix is set then hostnames without a\n# domain part have the suffix added as an optional domain part.\n# Example: In Proxmox, you create a Debian container with the hostname `test`.\n# This host would be available as `test.pve-internal.home.lkiesow.io`.\ndomain=pve-internal.protossnet.local\n\n# This configures local-only domains.\n# Queries in these domains are answered from /etc/hosts or DHCP only.\n# Queries for these domains are never forwarded to upstream names servers.\nlocal=/pve-internal.protossnet.local/\n\n# Listen on the given IP address(es) only to limit to what interfaces dnsmasq\n# should respond to.\n# This should be the IP address of your dnsmasq in your internal network.\nlisten-address=127.0.0.1\nlisten-address=10.0.0.2\n\n# Upstream DNS servers\n# We told dnsmasq to ignore the resolv.conf and thus need to explicitely specify\n# the upstream name servers. Use Cloudflare's and Google's DNS server or specify\n# a custom one.\n#server=1.1.1.1\n#server=8.8.8.8\n# Using the pi.hole deployed within Proxmox\nserver=192.168.1.6\n\n# DHCP range\n# Enable the DHCP server. Addresses will be given out from this range.\n# The following reserves 10.0.0.1 to 10.0.0.255 for static addresses not handled\n# by DHCP like the address for our Proxmox or dnsmasq server.\ndhcp-range=10.0.1.0,10.0.255.255\n\n# Set the advertised default route to the IP address of our Proxmox server.\ndhcp-option=option:router,10.0.0.1\n\n\n\n\n\n\n\nCareful with the .conf extension!\n\n\n\nLars doesn’t define an extension for the dnsmasq config file, but in Alpine distros this is the only extension accepted, so non-extension files are discarded. Bear this in mind when troubleshooting the service.\n\n\n\n\n\n\n\n\nMind the DHCP range!\n\n\n\nThe dhcp-range parameter is set between 10.0.1.0 and 10.0.255.255, meaning that the range between 10.0.0.1 and 10.0.0.255 is excluded. This is intentionally done, as they should be static addresses assigned to critical servers such as the Proxmox server (10.0.0.1) or the dnsmasq server itself (10.0.0.2).\n\n\n\n\n\n\n\n\nIf pointing to an upstream DNS server, what’s the purpose of this one, then?\n\n\n\nIn the dnsmasq config we define the server as our Pi-Hole server (192.168.1.6). This has upstream DNS servers that will resolve any domain name outside the internal network.\ndnsmasq provides resolution for the FQDN and hostnames within the internal network.\n\n\nFinally, restart and enable dnsmasq and check that it is up and running:\n# Both Debian and Alpine - Service restart\nservice dnsmasq restart\n# Debian - Enable service at boot time\nservice dnsmasq enable\n# Alpine - Add service at boot time\nrc-update add dnsmasq\n# Both Debian and Alpine - Service status\nservice dnsmasq.service status\n\n# Output for each command\n# Restart (Alpine)\n * Caching service dependencies ... [ ok ]\n * /var/lib/misc/dnsmasq.leases: creating file\n * /var/lib/misc/dnsmasq.leases: correcting owner\n * Starting dnsmasq ... [ ok ]\n# Enable (Alpine)\n * service dnsmasq added to runlevel default\n# Status (Alpine)\n * status: started"
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#test-the-set-up",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#test-the-set-up",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "Test the Set-Up",
    "text": "Test the Set-Up\nCreate two containers test-a and test-b, put them on the internal network by selecting vmbr1 as network bridge and set the IPv4 network configuration to DHCP. The logs in the dnsmasq server should show how it has assigned an IP to both containers (DHCPACK are the lines that prove it):\n\n\n/var/log/messages\n\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCP, IP range 10.0.1.0 -- 10.0.255.255, lease time 1h\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: using nameserver 192.168.1.6#53\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: using only locally-known addresses for pve-internal.protossnet.local\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: read /etc/hosts - 4 addresses\nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPREQUEST(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a\n# DHCPACK for test-a: IP assigned is 10.0.4.102\nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPACK(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a test-a\nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPREQUEST(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \n# DHCPACK for test-b: IP assigned is 10.0.61.210\nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPACK(eth0) 10.0.61.210 4e:70:ba:1e:73:ff test-b\n\nNow we should be able to ping to each other either by their IP or by their hostname/FQDN:\n\n\ntest-a --> test-b checks:\n\n# test-a pings test-b using IP\ntest-a:~# ping -c1 10.0.61.210\nPING 10.0.61.210 (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.072 ms\n\n--- 10.0.61.210 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.072/0.072/0.072 ms\n\n# test-a pings test-b using hostname\ntest-a:~# ping -c1 test-b\nPING test-b (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.063 ms\n\n--- test-b ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.063/0.063/0.063 ms\n\n# test-a pings test-b using FQDN\ntest-a:~# ping -c1 test-b.pve-internal.protossnet.local\nPING test-b.pve-internal.protossnet.local (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.047 ms\n\n--- test-b.pve-internal.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.047/0.047/0.047 ms\n\n\n\ntest-a <-- test-b checks:\n\n# test-a pings test-b using IP\ntest-b:~# ping -c1 10.0.4.102\nPING 10.0.4.102 (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.074 ms\n\n--- 10.0.4.102 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.074/0.074/0.074 ms\n\n# test-a pings test-b using hostname\nPING test-a (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.047 ms\n\n--- test-a ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.047/0.047/0.047 ms\n\n# test-a pings test-b using FQDN\ntest-b:~# ping -c1 test-a.pve-internal.protossnet.local\nPING test-a.pve-internal.protossnet.local (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.031 ms\n\n--- test-a.pve-internal.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.031/0.031/0.031 ms\n\n\n\n\n\n\n\nBonus check: ping outside devices\n\n\n\n\n\nThe vmbr1 interface is NATed, so any device within the original range of the Proxmox server IP should be reachable:\n\n\nExternal servers check:\n\n# Ping to Proxmox server through its external FQDN\ntest-b:~# ping -c1 pve.protossnet.local\nPING pve.protossnet.local (192.168.1.5): 56 data bytes\n64 bytes from 192.168.1.5: seq=0 ttl=64 time=0.041 ms\n\n--- pve.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.041/0.041/0.041 ms\n\n# Ping to PiHole through its given FQDN\ntest-b:~# ping -c1 pi.hole\nPING pi.hole (192.168.1.6): 56 data bytes\n64 bytes from 192.168.1.6: seq=0 ttl=63 time=0.176 ms\n\n--- pi.hole ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.176/0.176/0.176 ms\n\n# Ping to PiHole through its external FQDN\ntest-b:~# ping -c1 pi-hole.protossnet.local\nPING pi-hole.protossnet.local (192.168.1.6): 56 data bytes\n64 bytes from 192.168.1.6: seq=0 ttl=63 time=0.141 ms\n\n--- pi-hole.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.141/0.141/0.141 ms"
  },
  {
    "objectID": "homelab-project/virtualization-server/04-proxy-http-https-into-internal-network.html",
    "href": "homelab-project/virtualization-server/04-proxy-http-https-into-internal-network.html",
    "title": "Part 4 - Proxy HTTP(S) into the internal network",
    "section": "",
    "text": "We want to use Nginx as reverse proxy to make HTTP(S) requests to internal machines without noticing that they are actually on a private network.\n\n\n\nSomeone requests test-a.pve-internal.protossnet.local\n\nWe need to make sure the domain resolves to our Proxmox server (HOW???)\n\nIf HTTPS is being used, Nginx provides a valid TLS certificate\n\nWe need a wildcard certificate for *.pve-internal.protoss.local (HOW???)\n\nNginx proxies the request to the internal machine\n\nIt uses the same protocol that is being requested"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to my DevOps repository of guides and exercises. Here you’ll find (I hope) useful information about self-hosting your own services and managing them.\n\nLatest posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n00 - Kubernetes - Getting Started\n\n\n\n\n\n\n\nKubernetes\n\n\nAnsible\n\n\nk3s\n\n\n\n\nBuilding our first Kubernetes Cluster\n\n\n\n\n\n\nFeb 18, 2023\n\n\nProtossGP32\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\n00 - Containers - Docker & Docker Compose\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nContainers\n\n\n\n\nPre-requisites to deploy services as containers\n\n\n\n\n\n\nFeb 5, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n02 - Routing and SSL certificates - Traefik and Portainer\n\n\n\n\n\n\n\nDocker\n\n\nTraefik\n\n\nSSL\n\n\n\n\nRoute HTTP(s) requests to your services and secure them with Let’s Encrypt SSL certificates with Traefik\n\n\n\n\n\n\nFeb 5, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Authentication - OpenLDAP\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nOpenLDAP\n\n\nAuthentication\n\n\n\n\nCentralize your user database for services authentication\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Productivity tools - NextCloud\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nNextCloud\n\n\nProductivity tools\n\n\n\n\nUser your own productivity suite\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06 - Productivity tools - OpenProject\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nOpenProject\n\n\nProductivity tools\n\n\nProject Management\n\n\nScrum\n\n\n\n\nEfficiently manage your projects\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n07 - Code analysis - SonarQube\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nSonarQube\n\n\nCode analysis\n\n\n\n\nAnalyze your projects’ code and keep it well maintained\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - External access - Cloudflare\n\n\n\n\n\n\n\nDocker\n\n\nCloudflare\n\n\nSelf-hosted software\n\n\nContainers\n\n\nDNS\n\n\nSSL\n\n\n\n\nSecure remote access to your homelab\n\n\n\n\n\n\nJan 30, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03 - Homelab dashboard - Dashy\n\n\n\n\n\n\n\nDocker\n\n\nDashboard\n\n\nDashy\n\n\n\n\nAdd a centralized homepage for your services\n\n\n\n\n\n\nJan 30, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Provisioning - Virtual Machines on Proxmox\n\n\n\n\n\n\n\nProxmox VE\n\n\nVirtual Machines\n\n\n\n\nProvision new VMs in Proxmox\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Provisioning - LXC (Linux Containers on Proxmox)\n\n\n\n\n\n\n\nProxmox VE\n\n\nLXC\n\n\nContainers\n\n\n\n\nProvision new containers in Proxmox\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore California\n\n\n\n\n\n\n\nDocker\n\n\nSelenium\n\n\nAWS\n\n\nTerraform\n\n\nUnit tests\n\n\nIntegration tests\n\n\n\n\nStatic website debugging and deployment\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomelab Requirements\n\n\n\n\n\n\n\nDocker\n\n\nPortainer\n\n\nSelf-hosted software\n\n\nContainers\n\n\n\n\nThings you need to start building your own homelab\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomelab project introduction\n\n\n\n\n\n\n\nProxmox VE\n\n\nSelf-hosted software\n\n\n\n\nGreetings and motivation behind this project\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nPart 1 - Installing Proxmox VE\n\n\n\n\n\n\n\nProxmox VE\n\n\nVirtualisation\n\n\n\n\nInstall your own virtualisation server\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 - Internal Virtual Networks\n\n\n\n\n\n\n\nProxmox VE\n\n\nNetwork\n\n\n\n\nCreate internal network for your VM and LXC\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3 - DNS and DHCP servers\n\n\n\n\n\n\n\nProxmox VE\n\n\nNetwork\n\n\nDNS\n\n\nDHCP\n\n\n\n\nAutomatically assign IP and Domain Name to all your VM and LXC\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 4 - Proxy HTTP(S) into the internal network\n\n\n\n\n\n\n\nProxmox VE\n\n\nRemote access\n\n\nNetwork\n\n\nDNS\n\n\nProxy\n\n\nNginx\n\n\n\n\nMake your internal network accessible from the outside\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]