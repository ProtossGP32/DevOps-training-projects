[
  {
    "objectID": "devops-tools/infrastructure-as-a-code/00-ansible.html",
    "href": "devops-tools/infrastructure-as-a-code/00-ansible.html",
    "title": "00 - Infrastructure As A Code - Ansible",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "devops-tools/infrastructure-as-a-code/01-event-driven-ansible.html",
    "href": "devops-tools/infrastructure-as-a-code/01-event-driven-ansible.html",
    "title": "01 - Management As A Code - Event-Driven Ansible",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Marc Palacín and I’m a Data Engineer, Software Engineer and hopefully a future DevOps engineer!\nI love tinkering with electronics and hardware, and process automation is my passion."
  },
  {
    "objectID": "devops-exercises/explore-california/00-introduction.html",
    "href": "devops-exercises/explore-california/00-introduction.html",
    "title": "Explore California",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to my DevOps repository of guides and exercises. Here you’ll find (I hope) useful information about self-hosting your own services and managing them.\n\nLatest posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n08 - Remote access - Guacamole\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nGuacamole\n\n\nSSH\n\n\nRemote desktop\n\n\n\n\nAccess your servers via web browser\n\n\n\n\n\n\nFeb 28, 2023\n\n\nProtossGP32\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Kubernetes - Traefik + cert-manager\n\n\n\n\n\n\n\nKubernetes\n\n\nTraefik\n\n\ncert-manager\n\n\nk3s\n\n\n\n\nAdding reverse proxy and load balancing to our Kubernetes services\n\n\n\n\n\n\nFeb 19, 2023\n\n\nProtossGP32\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n00 - Kubernetes - Getting Started\n\n\n\n\n\n\n\nKubernetes\n\n\nAnsible\n\n\nk3s\n\n\n\n\nBuilding our first Kubernetes Cluster\n\n\n\n\n\n\nFeb 18, 2023\n\n\nProtossGP32\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\n00 - Containers - Docker & Docker Compose\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nContainers\n\n\n\n\nPre-requisites to deploy services as containers\n\n\n\n\n\n\nFeb 5, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\n02 - Routing and SSL certificates - Traefik and Portainer\n\n\n\n\n\n\n\nDocker\n\n\nTraefik\n\n\nSSL\n\n\n\n\nRoute HTTP(s) requests to your services and secure them with Let’s Encrypt SSL certificates with Traefik\n\n\n\n\n\n\nFeb 5, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n04 - Authentication - OpenLDAP\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nOpenLDAP\n\n\nAuthentication\n\n\n\n\nCentralize your user database for services authentication\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n05 - Productivity tools - NextCloud\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nNextCloud\n\n\nProductivity tools\n\n\n\n\nUser your own productivity suite\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n06 - Productivity tools - OpenProject\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nOpenProject\n\n\nProductivity tools\n\n\nProject Management\n\n\nScrum\n\n\n\n\nEfficiently manage your projects\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\n07 - Code analysis - SonarQube\n\n\n\n\n\n\n\nDocker\n\n\nSelf-hosted software\n\n\nSonarQube\n\n\nCode analysis\n\n\n\n\nAnalyze your projects’ code and keep it well maintained\n\n\n\n\n\n\nFeb 3, 2023\n\n\nProtossGP32\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - External access - Cloudflare\n\n\n\n\n\n\n\nDocker\n\n\nCloudflare\n\n\nSelf-hosted software\n\n\nContainers\n\n\nDNS\n\n\nSSL\n\n\n\n\nSecure remote access to your homelab\n\n\n\n\n\n\nJan 30, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n03 - Homelab dashboard - Dashy\n\n\n\n\n\n\n\nDocker\n\n\nDashboard\n\n\nDashy\n\n\n\n\nAdd a centralized homepage for your services\n\n\n\n\n\n\nJan 30, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 - Provisioning - Virtual Machines on Proxmox\n\n\n\n\n\n\n\nProxmox VE\n\n\nVirtual Machines\n\n\n\n\nProvision new VMs in Proxmox\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n02 - Provisioning - LXC (Linux Containers on Proxmox)\n\n\n\n\n\n\n\nProxmox VE\n\n\nLXC\n\n\nContainers\n\n\n\n\nProvision new containers in Proxmox\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore California\n\n\n\n\n\n\n\nDocker\n\n\nSelenium\n\n\nAWS\n\n\nTerraform\n\n\nUnit tests\n\n\nIntegration tests\n\n\n\n\nStatic website debugging and deployment\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomelab Requirements\n\n\n\n\n\n\n\nDocker\n\n\nPortainer\n\n\nSelf-hosted software\n\n\nContainers\n\n\n\n\nThings you need to start building your own homelab\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomelab project introduction\n\n\n\n\n\n\n\nProxmox VE\n\n\nSelf-hosted software\n\n\n\n\nGreetings and motivation behind this project\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nPart 1 - Installing Proxmox VE\n\n\n\n\n\n\n\nProxmox VE\n\n\nVirtualisation\n\n\n\n\nInstall your own virtualisation server\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2 - Internal Virtual Networks\n\n\n\n\n\n\n\nProxmox VE\n\n\nNetwork\n\n\n\n\nCreate internal network for your VM and LXC\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 3 - DNS and DHCP servers\n\n\n\n\n\n\n\nProxmox VE\n\n\nNetwork\n\n\nDNS\n\n\nDHCP\n\n\n\n\nAutomatically assign IP and Domain Name to all your VM and LXC\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 4 - Proxy HTTP(S) into the internal network\n\n\n\n\n\n\n\nProxmox VE\n\n\nRemote access\n\n\nNetwork\n\n\nDNS\n\n\nProxy\n\n\nNginx\n\n\n\n\nMake your internal network accessible from the outside\n\n\n\n\n\n\nJan 17, 2023\n\n\nProtossGP32\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homelab-project/virtualization-server/02-virtual-networks.html",
    "href": "homelab-project/virtualization-server/02-virtual-networks.html",
    "title": "Part 2 - Internal Virtual Networks",
    "section": "",
    "text": "Virtual networks are network interfaces that act as fake NICs and allow multiple VM or LXC to communicate within the same subnet. A virtual network can be bridged or bonded to a physical NIC or can live on its own without access to external devices (we’ll call them internal networks); the latter are useful when we have a cluster of resources that don’t need to reach external networks."
  },
  {
    "objectID": "homelab-project/virtualization-server/02-virtual-networks.html#build-a-proxmox-internal-network",
    "href": "homelab-project/virtualization-server/02-virtual-networks.html#build-a-proxmox-internal-network",
    "title": "Part 2 - Internal Virtual Networks",
    "section": "Build a Proxmox Internal Network",
    "text": "Build a Proxmox Internal Network\nFirst of all, we need to create a new network interface on our Proxmox server and assign it a network. This network bridge can then later be used to put machines on the internal network.\nThis gives us a fully functional internal network to use.\n\n\n\n\n\n\nMind the network transparency!\n\n\n\nAs Lars explains, the downside of having internal networks for clusters of VM is that we lose network transparency in that if a VM starts to behave erratically, its effect on the network level will be masked behind the Proxmox real IP, thus being unable to quickly identify the actual culprit.\nOn production environments, this would end in the IT department cutting down network access to the Proxmox server, and therefore to any VM machine inside it.\n\n\nAs a starting point, create a new network bridge in the Proxmox web interface:\n\nHead to Datacenter → Proxmox server name → Network, clic Create → Linux Bridge and set something like:\n\nName: vmbr1\nIPv4/CIDR: 10.0.0.1/16\nAutostart: ✓\nComment: Internal network\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis example provides an internal /16 network, more than enough for our inter machine configuration (65.536 IP addresses, being 65.534 usable!):\n\nNetwork Address: 10.0.0.0\nUsable IP range: 10.0.0.1 ~ 10.0.255.254\nBroadcast Address: 10.0.255.255\n\nIn most cases, using a /24 should be enough (253 IP addresses).\n\n\nThe file /etc/network/interfaces keeps the network interfaces configuration of the server. Once created the new interface, you should see the following new configuration block:\n\n\n/etc/network/interfaces\n\nauto vmbr1\niface vmbr1 inet static\n   address 10.0.0.1/16\n   bridge-ports none\n   bridge-stp off\n   bridge-fd 0\n\nOnce done, bring the new network interface up using:\nifup vmbr1\nAnd check its status by running:\nip a\n...\n4: vmbr1: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether 32:7b:7f:8b:f8:9c brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.1/16 scope global vmbr1\n       valid_lft forever preferred_lft forever\nWith this, we have a fully functional network interface for our internal VMs and containers, but without access to the outside world.\n\n\n\n\n\n\nWhat about the state DOWN?\n\n\n\nThat’s because there’s no host using it yet. What’s important is that the inet is correctly configured as 10.0.0.1/16.\nOnce we attach this interface to some containers or VM, its state will change to UP.\n\n\n\n\n\n\n\n\nCommenting on Networks\n\n\n\nLars has a point when mentioning the importance of giving proper comments to each network interface. While it might seem unnecessary when dealing with just a few interfaces, we can forget about it once the number of appliances and services grow.\nThus, the default vmbr0 created on Proxmox installation time and with proper access to the outside shall be named External network, while the new vmbr1 shall be called Internal network. This way we avoid any kind of doubt when picking one of them. Use similar criteria with any new network interface.\n\n\n\nNAT Configuration\nOur current network interface vmbr1 can be useful for appliances that don’t require to access the internet (i.e. servers that simply execute local tasks and talk to each other). If we want this network interface to be able to reach the outside world, we need a NAT configuration.\n\n\n\n\n\n\nNAT\n\n\n\nNAT is the process where all traffic from a network is routed through a single IP address outside that network, and then return results to the request source.\n\n\nIn order to configure NAT in our interface, edit the file /etc/network/interface and add the following lines to the vmbr1 config block:\n\n\n/etc/network/interface\n\niface vmbr1 inet static\n   ...\n   post-up   echo 1 > /proc/sys/net/ipv4/ip_forward\n   post-up   iptables -t nat -A POSTROUTING -s '10.0.0.0/16' -o vmbr0 -j MASQUERADE\n   post-down iptables -t nat -D POSTROUTING -s '10.0.0.0/16' -o vmbr0 -j MASQUERADE\n\nWhat we are doing here is add rules to the interface, telling it how to route the packages in that network. This rule will create a dynamic source NAT where every packet from 10.0.0.0/16 will be sent to the interface vmbr0 (the External network) and the source IP address of this packet will be replaced by the primary IP of vmbr0 (the Proxmox server IP).\n\nSetting 1 to /proc/sys/net/ipv4/ip_forward enables to forward packages to another interface (TODO: needs references)\nThe -t nat -A POSTROUTING rule means that it allows the vmbr0 interface to route packages from the source -s '10.0.0.0/16', only after the interface vmbr1 is UP, that’s why it is defined as a post-up rule\nThe -t nat -D POSTROUTING rule does the opposite, denies any routing of packages from source -s '10.0.0.0' once the vmbr1 interface is DOWN, ensuring that no unexpected packages are routed through vmbr0\n\n\n\nTesting your Configuration\nFirst, make sure that both interfaces are up, either by executing ifup <interface name> or by rebooting the whole server.\nNext setup a simple container or virtual machine, assign vmbr1 interface as its Network interface and statically configure its IP and gateway according to the interface range. For example:\n\nBridge: vmbr1\nIPv4/CIDR: 10.0.0.2/16\nGateway (IPv4): 10.0.0.1\n\nOnce started, ssh into that server/container and try pinging any external server IP. Remember that there’s no DNS server for this network, so we can’t use domain names yet:\n\n\nPing test:\n\n# Let's try pinging Google\nping -4 -c 1 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=119 time=18.8 ms\n\n--- 8.8.8.8 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 18.764/18.764/18.764/0.000 ms\n\nThe ping command has returned results, so the network has access to external servers!"
  },
  {
    "objectID": "homelab-project/virtualization-server/04-proxy-http-https-into-internal-network.html",
    "href": "homelab-project/virtualization-server/04-proxy-http-https-into-internal-network.html",
    "title": "Part 4 - Proxy HTTP(S) into the internal network",
    "section": "",
    "text": "We want to use Nginx as reverse proxy to make HTTP(S) requests to internal machines without noticing that they are actually on a private network.\n\n\n\nSomeone requests test-a.pve-internal.protossnet.local\n\nWe need to make sure the domain resolves to our Proxmox server (HOW???)\n\nIf HTTPS is being used, Nginx provides a valid TLS certificate\n\nWe need a wildcard certificate for *.pve-internal.protoss.local (HOW???)\n\nNginx proxies the request to the internal machine\n\nIt uses the same protocol that is being requested"
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "",
    "text": "TODO\n\n\n\nPending to describe Proxmox"
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html#ip-address-assigned-via-dhcp",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html#ip-address-assigned-via-dhcp",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "IP address assigned via DHCP",
    "text": "IP address assigned via DHCP\nThis way the DHCP server will assign the correct domain name and the server will be accessible by its domain name.\nA server should always have a predictable IP, so configure the DHCP server to lease the same IP to the Proxmox server network MAC address.\n\n\n\n\n\n\nTODO\n\n\n\nPending to elaborate\n\n\n\n\n\n\n\n\nRegarding DNS and DHCP servers…\n\n\n\nLater on, we’ll configure our own virtual DHCP and DNS servers to provide internal IPs to our VM and containers."
  },
  {
    "objectID": "homelab-project/virtualization-server/01-installing-proxmox.html#allow-https-on-port-443-via-nginx",
    "href": "homelab-project/virtualization-server/01-installing-proxmox.html#allow-https-on-port-443-via-nginx",
    "title": "Part 1 - Installing Proxmox VE",
    "section": "Allow HTTPS on Port 443 via Nginx",
    "text": "Allow HTTPS on Port 443 via Nginx\nAs Lars mentions in his guide, the web interface of Proxmox is available on port 8006 instead of the default HTTPS port 443, and HTTP on Port 80 is not available at all.\nHaving HTTPS acesses through ports different than 443 can be very bothersome as we need to remember what exact port is the one used on each service.\nIn order to solve this, we’ll use a light-weight web server that also acts as a reverse proxy natively in the Proxmox server, so we can redirect 443 requests to the 8006 port.\n\n\n\n\n\n\nAbout Nginx…\n\n\n\nLater on we’ll be also deploying other Nginx servers within our internal networks to redirect traffic to each one of our services\n\n\n\nInstall and Configure Nginx\nConnect via SSH to the PVE server as root:\nssh root@<proxmox-ip>\n# or\nssh root@<proxmox-hostname>.<local-domain>\nInstall the nginx-light package:\napt-get install nginx-light\nA web server should already be up and running. Verify it by accessing http://<proxmox-ip>/ or http://<proxmox-hostname>.<local-domain>, a blank welcome screen should appear.\n\n\n\nNginx welcome web page\n\n\nOnce done, remove the default Nginx configuration from the Proxmox server…\nrm /etc/nginx/sites-*/default\n…and create a new file /etc/nginx/sites-available/proxmox-web-interface with the following content:\n\n\n/etc/nginx/sites-available/proxmox-web-interface\n\nserver {\n  # Enforce HTTPS by redirecting requests\n  listen 80;\n  listen [::]:80;\n  server_name pve.protossnet.local;\n\n  location / {\n    return 301 https://pve.protossnet.local$request_uri;\n  }\n}\n\nserver {\n  listen 443 ssl http2;\n  listen [::]:443 ssl http2;\n  server_name pve.protossnet.local;\n\n  ssl_certificate_key /etc/pve/local/pve-ssl.key;\n  ssl_certificate     /etc/pve/local/pve-ssl.pem;\n\n  # Proxy configuration\n  location / {\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_pass https://127.0.0.1:8006;\n    proxy_buffering off;\n    client_max_body_size 0;\n    proxy_connect_timeout  3600s;\n    proxy_read_timeout  3600s;\n    proxy_send_timeout  3600s;\n    send_timeout  3600s;\n    proxy_set_header Host $host;\n    proxy_ssl_name $host;\n    proxy_set_header X-Forwarded-For $remote_addr;\n  }\n}\n\n\n\n\n\n\n\nNote\n\n\n\nLars has additional and more complex Nginx configs for the web interface, it’s worth checking them out to learn about how Nginx works and how powerful it can be!\n\n\nAfter updating the configuration, we must create a softlink from the sites-enabled directory to make it available:\ncd /etc/nginx/sites-enabled\nln -s /etc/nginx/sites-available/proxmox-web-interface\nFinally, we check if Nginx can spot any errors:\nnginx -t\n\n\nRestart Nginx and Enable it by Default\nTo start Nginx and make sure it starts automatically after a system reboot, run:\n# Restart the Nginx service\nsystemctl restart nginx.service\n# Permanently enable the Nginx service\nsystemctl enable nginx.service\nFinally, make sure that Nginx will start only after Proxmox starts, since the certificates may otherwise not be available yet. To do that, create a file /etc/systemd/system/nginx.service.d/override.conf with the content:\n[Unit]\nRequires=pve-cluster.service\nAfter=pve-cluster.service\nAlternatively, you can also use systemctl edit nginx.service to edit this file.\nWith the service restarted, check that we can access the Proxmox VE web interface through the expected HTTPS port by going to https://<proxmox-hostname>.<local-domain> or https://<proxmox-ip>:\n\n\n\nProxmox - Nginx correctly redirecting HTTPS request\n\n\n\n\n\n\n\n\nWhat about the insecure HTTPS warning?\n\n\n\nRight now Nginx is using the SSL certificates provided by Proxmox on installation time. We should provide valid SSL certifications to avoid this, such as the ones that Let’s Encript can generate.\n\n\n\n\nMake Proxmox UI Service Listen to Localhost Only\n\n\n\n\n\n\nLars suggests this change to ensure that everyone is using the Nginx set-up, but I don’t really understand what exactly means. I’ve applied the change and at least I can still access it from within my local network.\n\n\n\nCreate /etc/default/pveproxy and set:\nLISTEN_IP=\"127.0.0.1\"\nThen restart the pveproxy service:\nsystemctl restart pveproxy.service"
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "",
    "text": "Wait, more DHCP servers? Why?\n\n\n\nYou might be thinking about the DHCP server that provides the IP address to the Proxmox server and wondering why this server can’t handle the IP assignment of the future VMs. Here’s why:\nA DHCP server only serves IPs within a certain established range, and we might not have access to it (restricted MAC access, limited number of IPs, etc…). Furthermore, we’ve created an internal network interface whose IP range shouldn’t match with the upstream server and is also masquerading its IPs behind the Proxmox server one (remember NAT?). Having said that, the next course of action is to create custom DHCP and DNS servers for each internal network.\nLater on, we’ll discuss how to allow multiple internal networks to talk to each other, but for now we’ll only use one.\nWe’ll be using dnsmasq to host a DHCP and DNS server either on the Proxmox server itself or on a machine or container running in Proxmox. What approach is better? It always depends on your needs:\nIn order to touch the least the Proxmox internal configurations and avoid messing with other active DHCP servers, we’ll go with the Container route. If you want to go with the Proxmox Server set-up, Lars also explains it here."
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#using-a-container",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#using-a-container",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "Using a container",
    "text": "Using a container\n\n\n\n\n\n\nContainer creation guide\n\n\n\nLXC - Linux Containers on Proxmox\n\n\nCreate a Debian container with 128 MB of RAM and assign it a static IPv4 address on the internal network (vmbr1 or check the interface comments). Make sure the IPv4 address is unique and does not clash with already assigned addresses. As DNS and DHCP services are quite important in a network, it’s usual to assign them the first or the last usable IP within the range; as we’re using 10.0.0.1 as our gateway (the Proxmox server), the next available one is 10.0.0.2:\n\nBridge: vmbr1\nIPv4: Static\nIPv4/CIDR: 10.0.0.2/16\nGateway (IPv4): 10.0.0.1\n\n\n\n\n\n\n\nAlpine instead of Debian\n\n\n\nFor the sake of using even more minimal distributions, I’ve used Alpine as it is very lightweigh. When using it, just remember that its package manager is apk instead of apt, and that installing new packages is done with add instead of install.\n\n\nMake sure to update the container as usual:\n# Alpine\napk update\napk upgrade\n\n# Debian\napt update\napt upgrade\nDebian only! The Proxmox Debian container template comes with systemd-resolved enabled. This service conflicts with dnsmasq and we don’t actually need it, so we disable it:\nsystemctl stop systemd-resolved.service\nsystemctl disable systemd-resolved.service\nNow we install dnsmasq:\n# Alpine\napk add dnsmasq\n\n# Debian\napt install dnsmasq\nAs usual, Debian will automatically start the service after its installation (this doesn’t happen in Alpine). But by default dnsmasq is configured as a DNS server only, so we need to add additional configuration to enable the DHCP feature. In any case, create a file /etc/dnsmasq.d/internal.conf with the following configuration:\n\n\n/etc/dnsmasq.d/internal.conf\n\n# Tells dnsmasq to never forward A or AAAA queries for plain names,\n# without dots or domain parts, to upstream nameservers.\n# If the name is not known from /etc/hosts or DHCP then a \"not found\" answer is\n# returned.\ndomain-needed\n\n# All reverse lookups for private IP ranges (ie 192.168.x.x, etc) which are not\n# found in /etc/hosts or the DHCP leases file are answered with \"no such domain\"\n# rather than being forwarded upstream.\nbogus-priv\n\n# Don't read /etc/resolv.conf. Get upstream servers only from the command line\n# or the dnsmasq configuration file.\nno-resolv\n\n# Later  versions of windows make periodic DNS requests which don't get sensible\n# answers from the public DNS and can cause problems by triggering dial-on-\n# demand links. This flag turns on an option to filter such requests. The\n# requests blocked are for records of types SOA and SRV, and type  ANY  where\n# the requested name has underscores, to catch LDAP requests.\nfilterwin2k\n\n# Add the domain to simple names (without a period) in /etc/hosts in the same\n# way as for DHCP-derived names. Note that this does not apply to domain names\n# in cnames, PTR records, TXT records etc.\nexpand-hosts\n\n# Specifies DNS domains for the DHCP server. This  has  two  effects;\n# firstly it causes the DHCP server to return the domain to any hosts which\n# request it, and secondly it sets the domain which is legal for DHCP-configured\n# hosts to claim.  In addition, when a suffix is set then hostnames without a\n# domain part have the suffix added as an optional domain part.\n# Example: In Proxmox, you create a Debian container with the hostname `test`.\n# This host would be available as `test.pve-internal.home.lkiesow.io`.\ndomain=pve-internal.protossnet.local\n\n# This configures local-only domains.\n# Queries in these domains are answered from /etc/hosts or DHCP only.\n# Queries for these domains are never forwarded to upstream names servers.\nlocal=/pve-internal.protossnet.local/\n\n# Listen on the given IP address(es) only to limit to what interfaces dnsmasq\n# should respond to.\n# This should be the IP address of your dnsmasq in your internal network.\nlisten-address=127.0.0.1\nlisten-address=10.0.0.2\n\n# Upstream DNS servers\n# We told dnsmasq to ignore the resolv.conf and thus need to explicitely specify\n# the upstream name servers. Use Cloudflare's and Google's DNS server or specify\n# a custom one.\n#server=1.1.1.1\n#server=8.8.8.8\n# Using the pi.hole deployed within Proxmox\nserver=192.168.1.6\n\n# DHCP range\n# Enable the DHCP server. Addresses will be given out from this range.\n# The following reserves 10.0.0.1 to 10.0.0.255 for static addresses not handled\n# by DHCP like the address for our Proxmox or dnsmasq server.\ndhcp-range=10.0.1.0,10.0.255.255\n\n# Set the advertised default route to the IP address of our Proxmox server.\ndhcp-option=option:router,10.0.0.1\n\n\n\n\n\n\n\nCareful with the .conf extension!\n\n\n\nLars doesn’t define an extension for the dnsmasq config file, but in Alpine distros this is the only extension accepted, so non-extension files are discarded. Bear this in mind when troubleshooting the service.\n\n\n\n\n\n\n\n\nMind the DHCP range!\n\n\n\nThe dhcp-range parameter is set between 10.0.1.0 and 10.0.255.255, meaning that the range between 10.0.0.1 and 10.0.0.255 is excluded. This is intentionally done, as they should be static addresses assigned to critical servers such as the Proxmox server (10.0.0.1) or the dnsmasq server itself (10.0.0.2).\n\n\n\n\n\n\n\n\nIf pointing to an upstream DNS server, what’s the purpose of this one, then?\n\n\n\nIn the dnsmasq config we define the server as our Pi-Hole server (192.168.1.6). This has upstream DNS servers that will resolve any domain name outside the internal network.\ndnsmasq provides resolution for the FQDN and hostnames within the internal network.\n\n\nFinally, restart and enable dnsmasq and check that it is up and running:\n# Both Debian and Alpine - Service restart\nservice dnsmasq restart\n# Debian - Enable service at boot time\nservice dnsmasq enable\n# Alpine - Add service at boot time\nrc-update add dnsmasq\n# Both Debian and Alpine - Service status\nservice dnsmasq.service status\n\n# Output for each command\n# Restart (Alpine)\n * Caching service dependencies ... [ ok ]\n * /var/lib/misc/dnsmasq.leases: creating file\n * /var/lib/misc/dnsmasq.leases: correcting owner\n * Starting dnsmasq ... [ ok ]\n# Enable (Alpine)\n * service dnsmasq added to runlevel default\n# Status (Alpine)\n * status: started"
  },
  {
    "objectID": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#test-the-set-up",
    "href": "homelab-project/virtualization-server/03-dns-dhcp-servers.html#test-the-set-up",
    "title": "Part 3 - DNS and DHCP servers",
    "section": "Test the Set-Up",
    "text": "Test the Set-Up\nCreate two containers test-a and test-b, put them on the internal network by selecting vmbr1 as network bridge and set the IPv4 network configuration to DHCP. The logs in the dnsmasq server should show how it has assigned an IP to both containers (DHCPACK are the lines that prove it):\n\n\n/var/log/messages\n\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCP, IP range 10.0.1.0 -- 10.0.255.255, lease time 1h\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: using nameserver 192.168.1.6#53\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: using only locally-known addresses for pve-internal.protossnet.local\nJan 16 22:57:35 vmbr1-dnsmasq daemon.info dnsmasq[741]: read /etc/hosts - 4 addresses\nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a \nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPREQUEST(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a\n# DHCPACK for test-a: IP assigned is 10.0.4.102\nJan 16 22:57:42 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPACK(eth0) 10.0.4.102 c6:d8:bc:22:dc:4a test-a\nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPDISCOVER(eth0) 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPOFFER(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPREQUEST(eth0) 10.0.61.210 4e:70:ba:1e:73:ff \n# DHCPACK for test-b: IP assigned is 10.0.61.210\nJan 16 22:57:48 vmbr1-dnsmasq daemon.info dnsmasq-dhcp[741]: DHCPACK(eth0) 10.0.61.210 4e:70:ba:1e:73:ff test-b\n\nNow we should be able to ping to each other either by their IP or by their hostname/FQDN:\n\n\ntest-a --> test-b checks:\n\n# test-a pings test-b using IP\ntest-a:~# ping -c1 10.0.61.210\nPING 10.0.61.210 (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.072 ms\n\n--- 10.0.61.210 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.072/0.072/0.072 ms\n\n# test-a pings test-b using hostname\ntest-a:~# ping -c1 test-b\nPING test-b (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.063 ms\n\n--- test-b ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.063/0.063/0.063 ms\n\n# test-a pings test-b using FQDN\ntest-a:~# ping -c1 test-b.pve-internal.protossnet.local\nPING test-b.pve-internal.protossnet.local (10.0.61.210): 56 data bytes\n64 bytes from 10.0.61.210: seq=0 ttl=64 time=0.047 ms\n\n--- test-b.pve-internal.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.047/0.047/0.047 ms\n\n\n\ntest-a <-- test-b checks:\n\n# test-a pings test-b using IP\ntest-b:~# ping -c1 10.0.4.102\nPING 10.0.4.102 (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.074 ms\n\n--- 10.0.4.102 ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.074/0.074/0.074 ms\n\n# test-a pings test-b using hostname\nPING test-a (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.047 ms\n\n--- test-a ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.047/0.047/0.047 ms\n\n# test-a pings test-b using FQDN\ntest-b:~# ping -c1 test-a.pve-internal.protossnet.local\nPING test-a.pve-internal.protossnet.local (10.0.4.102): 56 data bytes\n64 bytes from 10.0.4.102: seq=0 ttl=64 time=0.031 ms\n\n--- test-a.pve-internal.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.031/0.031/0.031 ms\n\n\n\n\n\n\n\nBonus check: ping outside devices\n\n\n\n\n\nThe vmbr1 interface is NATed, so any device within the original range of the Proxmox server IP should be reachable:\n\n\nExternal servers check:\n\n# Ping to Proxmox server through its external FQDN\ntest-b:~# ping -c1 pve.protossnet.local\nPING pve.protossnet.local (192.168.1.5): 56 data bytes\n64 bytes from 192.168.1.5: seq=0 ttl=64 time=0.041 ms\n\n--- pve.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.041/0.041/0.041 ms\n\n# Ping to PiHole through its given FQDN\ntest-b:~# ping -c1 pi.hole\nPING pi.hole (192.168.1.6): 56 data bytes\n64 bytes from 192.168.1.6: seq=0 ttl=63 time=0.176 ms\n\n--- pi.hole ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.176/0.176/0.176 ms\n\n# Ping to PiHole through its external FQDN\ntest-b:~# ping -c1 pi-hole.protossnet.local\nPING pi-hole.protossnet.local (192.168.1.6): 56 data bytes\n64 bytes from 192.168.1.6: seq=0 ttl=63 time=0.141 ms\n\n--- pi-hole.protossnet.local ping statistics ---\n1 packets transmitted, 1 packets received, 0% packet loss\nround-trip min/avg/max = 0.141/0.141/0.141 ms"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/01-installing-traefik.html",
    "href": "homelab-project/kubernetes-cluster/01-installing-traefik.html",
    "title": "01 - Kubernetes - Traefik + cert-manager",
    "section": "",
    "text": "Now that we have our K3S cluster deployed and some apps running, the next logical step is to make them accessible. We’re going to install the following components to our cluster for this:\n\nTraefik: Traefik acts as both a load balancer and a reverse proxy that makes deploying microservices easy\ncert-manager: cert-manager is a powerful and extensive X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-helm",
    "href": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-helm",
    "title": "01 - Kubernetes - Traefik + cert-manager",
    "section": "Installing Helm",
    "text": "Installing Helm\nHelm is an executable which is implemented into two distinct parts:\n\nHelm Client: command-line client for end users. It is responsible for the following:\n\nLocal chart development\nManaging repositories\nManaging releases\nInterfacing with the Helm library\n\nSending charts to be installed\nRequesting upgrading or uninstalling of existing releases\n\n\nHelm Library: it provides the logic for executing all Helm operations. It interfaces with the Kubernetes API serer and provides the following capability:\n\nCombining a chart and configuration to build a release\nInstalling charts into Kubernetes, and providing the subsequent release object\nUpgrading and uninstalling charts by interacting with Kubernetes\n\n\nThe following commands automate the helm installation:\n\n\n\n\n\n\nInstall it on a server with access to the k3s cluster!\n\n\n\nIf the server doesn’t have the kubectl binary and access to the cluster, Helm won’t be able to communicate with the Kubernetes API\n\n\nWe’ll install helm on the LXC Alpine container where we previously installed ansible and kubectl:\n\n\nHelm installation\n\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n# Use `sh` instead of `bash` in alpine\nsh get_helm.sh\n\n\n\n\n\n\n\nFailure on helm download script\n\n\n\nIn Alpine, we don’t have bash shell, so we have to execute the script using sh; thus, some commands don’t execute as expected. Executing as sh -x get_helm.sh, we see that the error is in the comparison of the result of the runAsRoot function:\n+ runAsRoot cp /tmp/helm-installer-bOIFjI/helm/linux-amd64/helm /usr/local/bin/helm\n+ '[' -ne 0 -a true '=' true ]\nsh: 0: unknown operand\nEven though the verification fails, the binary is correctly installed:\nls -l /usr/local/bin/helm\n-rwxr-xr-x    1 root     root      46870528 Feb 19 12:31 /usr/local/bin/helm\n\n\nVerify once again that we can reach the k3s cluster and that helm is correctly installed:\n\n\nHelm verification\n\nkubectl get nodes\nNAME            STATUS   ROLES                       AGE   VERSION\nk3s-control-1   Ready    control-plane,etcd,master   8h    v1.24.10+k3s1\nk3s-control-2   Ready    control-plane,etcd,master   8h    v1.24.10+k3s1\nk3s-control-3   Ready    control-plane,etcd,master   8h    v1.24.10+k3s1\nk3s-worker-1    Ready    <none>                      8h    v1.24.10+k3s1\nk3s-worker-2    Ready    <none>                      8h    v1.24.10+k3s1\n\nhelm version\nversion.BuildInfo{Version:\"v3.11.1\", GitCommit:\"293b50c65d4d56187cd4e2f390f0ada46b4c4737\", GitTreeState:\"clean\", GoVersion:\"go1.18.10\"}\n\n\n\n\n\n\n\nClone the resources!\n\n\n\nFrom now on, we’ll be deploying components using the resources that the guide provides to us in this github repository. Make sure to clone them into the Helm server!\nYou can use this StackOverflow response to only checkout a subfolder of the whole repository."
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-traefik",
    "href": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-traefik",
    "title": "01 - Kubernetes - Traefik + cert-manager",
    "section": "Installing Traefik",
    "text": "Installing Traefik\nNow we’ll use Helm to install Traefik. First of all, modify the kubernetes/traefik-cert-manager/traefik/values.yaml so the service.spec.loadBalancerIP points towards an available IP within the MetalLB range:\n\n\nvalues.yaml\n\nservice:\n    [...]\n    spec:\n        loadBalancerIP: 10.0.0.12 # We use .12 as .11 is currently used by the nginx example\n\nThen, follow these steps to add the traefik repo and install it:\n\n\nTraefik installation\n\n# Add the traefik repository\n$ helm repo add traefik https://helm.traefik.io/traefik\n\"traefik\" has been added to your repositories\n\n# Update the traefik repository\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"traefik\" chart repository\nUpdate Complete. ⎈Happy Helming!⎈\n\n# Create a new namespace for traefik\n$ kubectl create namespace traefik\nnamespace/traefik created\n\n# Get all namespaces\n$ kubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   8h\nkube-node-lease   Active   8h\nkube-public       Active   8h\nkube-system       Active   8h\nmetallb-system    Active   8h\ntraefik           Active   31s\n\n# Now install traefik using the values defined in the resources. We must be on the same folder than the 'values.yaml' file\n$ cd kubernetes/traefik-cert-manager/traefik\n# Launch the installation\n$ helm install --namespace=traefik traefik traefik/traefik --values=values.yaml\nNAME: traefik\nLAST DEPLOYED: Sun Feb 19 13:05:36 2023\nNAMESPACE: traefik\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nTraefik Proxy v2.9.7 has been deployed successfully\non traefik namespace !\n\nOnce installed, check the status of the traefik service:\n\n\nTraefik service status\n\n$ kubectl get svc --all-namespaces -o wide\nNAMESPACE        NAME              TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE    SELECTOR\ndefault          kubernetes        ClusterIP      10.43.0.1       <none>        443/TCP                      8h     <none>\ndefault          nginx             LoadBalancer   10.43.170.67    10.0.0.11     80:30466/TCP                 8h     app=nginx\nkube-system      kube-dns          ClusterIP      10.43.0.10      <none>        53/UDP,53/TCP,9153/TCP       8h     k8s-app=kube-dns\nkube-system      metrics-server    ClusterIP      10.43.160.192   <none>        443/TCP                      8h     k8s-app=metrics-server\nmetallb-system   webhook-service   ClusterIP      10.43.41.163    <none>        443/TCP                      8h     component=controller\ntraefik          traefik           LoadBalancer   10.43.50.42     10.0.0.12     80:32607/TCP,443:32263/TCP   113s   app.kubernetes.io/instance=traefik-traefik,app.kubernetes.io/name=traefik\n\nTraefik is now accessible through 10.0.0.11:80\n\n\n\n\n\n\nDashboard enabled but not accessible yet\n\n\n\nWe need to create an Ingress rule to be able to access it. More on this later on.\n\n\nCheck the pods currently running on the traefik namespace:\n\n\nChecking Traefik pods\n\n$ kubectl get pods --namespace traefik -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP          NODE           NOMINATED NODE   READINESS GATES\ntraefik-f775dc87d-8q5vq   1/1     Running   0          3m42s   10.42.3.4   k3s-worker-1   <none>           <none>\ntraefik-f775dc87d-9z9kr   1/1     Running   0          3m42s   10.42.4.4   k3s-worker-2   <none>           <none>\ntraefik-f775dc87d-phgh6   1/1     Running   0          3m42s   10.42.3.5   k3s-worker-1   <none>           <none>\n\nWe can see that we have 3 replicas of traefik running on different worker nodes."
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-traefik-middleware-for-default-headers",
    "href": "homelab-project/kubernetes-cluster/01-installing-traefik.html#installing-traefik-middleware-for-default-headers",
    "title": "01 - Kubernetes - Traefik + cert-manager",
    "section": "Installing Traefik Middleware for default headers",
    "text": "Installing Traefik Middleware for default headers\nWhat is Traefik Middleware? Middleware is a means of tweaking the requests before they are sent to our services (of before the answer from the services are sent to the clients). This is critical as Traefik acts as a router between the client and the services running in our k3s cluster, thus things like the requests’ headers, authentication, etc. must be modified so they can correctly reach their destination.\nWe’ll proceed to install the middleware now. Make sure you are in the same directory as before, where values.yaml and default-headers.yaml files are:\n\n\nMiddleware installation\n\n$ kubectl apply -f default-headers.yaml\nmiddleware.traefik.containo.us/default-headers created\n\nSome middleware checks:\n\n\nMiddleware checks\n\n$ kubectl get middleware\nNAME              AGE\ndefault-headers   46s\n\n$kubectl describe middleware default-headers\nName:         default-headers\nNamespace:    default\nLabels:       <none>\nAnnotations:  <none>\nAPI Version:  traefik.containo.us/v1alpha1\nKind:         Middleware\nMetadata:\n  Creation Timestamp:  2023-02-19T13:22:04Z\n  Generation:          1\n  Managed Fields:\n    API Version:  traefik.containo.us/v1alpha1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:kubectl.kubernetes.io/last-applied-configuration:\n      f:spec:\n        .:\n        f:headers:\n          .:\n          f:browserXssFilter:\n          f:contentTypeNosniff:\n          f:customFrameOptionsValue:\n          f:customRequestHeaders:\n            .:\n            f:X-Forwarded-Proto:\n          f:forceSTSHeader:\n          f:stsIncludeSubdomains:\n          f:stsPreload:\n          f:stsSeconds:\n    Manager:         kubectl-client-side-apply\n    Operation:       Update\n    Time:            2023-02-19T13:22:04Z\n  Resource Version:  122070\n  UID:               c8e92a6a-5e0d-4aba-9563-72363dd2606e\nSpec:\n  Headers:\n    Browser Xss Filter:          true\n    Content Type Nosniff:        true\n    Custom Frame Options Value:  SAMEORIGIN\n    Custom Request Headers:\n      X - Forwarded - Proto:  https\n    Force STS Header:         true\n    Sts Include Subdomains:   true\n    Sts Preload:              true\n    Sts Seconds:              15552000\nEvents:                       <none>"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/01-installing-traefik.html#enable-traefik-dashboard-access",
    "href": "homelab-project/kubernetes-cluster/01-installing-traefik.html#enable-traefik-dashboard-access",
    "title": "01 - Kubernetes - Traefik + cert-manager",
    "section": "Enable Traefik Dashboard access",
    "text": "Enable Traefik Dashboard access\nIn order to properly manage Traefik, we’ll enable an Ingress rule towards the Dashboard that will give us access to create and monitor all routes to our services:\n\nCreate basic auth credentials\nFirst of all, install some required apache utils for password generation:\n\n\nApache2 utils\n\n# Alpine\n$ apk add apache2-utils\n\nCreate a username/password credentials encoded in base64:\n\n\nCreating credentials\n\n$ htpasswd -nb <username> <password> | openssl base64\n# The resulting string is the encoded credentials\n\nGo to the Dashboard folder and follow these steps:\n\nModify the dashboard/secret-dashboard.yaml file to include the previously generated credentials\n\n\n\nsecret-dashboard.yaml\n\n[...]\ndata:\n    users: <your-base64-newly-created credentials\n\n\nApply the secret to make it available in the traefik namespace:\n\n\n\nDashboard Secret applying\n\n$ kubectl apply -f secret-dashboard.yaml\nsecret/traefik-dashboard-auth created\n\n\nCheck that the secrets have been created:\n\n\n\nSecret check\n\n$ kubectl get secrets --namespace traefik\nNAME                            TYPE                 DATA   AGE\nsh.helm.release.v1.traefik.v1   helm.sh/release.v1   1      33m\ntraefik-dashboard-auth          Opaque               1      63s\n\n$ kubectl -n traefik describe secret traefik-dashboard-auth\nName:         traefik-dashboard-auth\nNamespace:    traefik\nLabels:       <none>\nAnnotations:  <none>\n\nType:  Opaque\n\nData\n====\nusers:  47 bytes\n\n\n\nCreate Dashboard middleware for basic authentication\nTraefik dashboard needs another middleware to deal with the authentication process. Just deploy the middleware.yaml file, you don’t have to modify anything from it:\n\n\nDeploy dashboard auth middleware and check\n\n$ kubectl apply -f middleware.yaml\nmiddleware.traefik.containo.us/traefik-dashboard-basicauth created\n\n$ kubectl -n traefik get middleware\nNAME                          AGE\ntraefik-dashboard-basicauth   12s\n\n\n\nCreate Ingress rules to access TraefikService API through a hostname\nYou also have to make sure that the Traefik service IP (the one we configured in the values.yaml and that is part of the MetalLB IP range) is accessible through a hostname. We must create a DNS record for this in our DNS server (PiHole)\n\n\n\n\n\n\nTODO: Explain how to configure the DNS record towards an internal network and how to check it\n\n\n\n\n\n\nAfter that is done, modify the ingress.yaml file accordingly:\n\n\ningress.yaml\n\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`traefik.pve-internal.protossnet.local`) # This is where we put the hostname defined in the DNS record\n\nApply the YAML file as always:\n\n\nDeploying Traefik Ingress rule\n\n$ kubectl apply -f ingress.yaml\ningressroute.traefik.containo.us/traefik-dashboard created\n\nNow we should be able to access the web dashboard at the following address: https://traefik.pve-internal.protossnet.local\n\n\n\n\n\n\nCheck DHCP and DNS configuration for internal network\n\n\n\nPiHole doesn’t seem to redirect requests to the internal network!\n\n\n\n\n\n\n\n\nCOMING SOON\n\n\n\nAdd the cert-manager component and configure Traefik to provide certificates to our kubernetes services"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#main-concepts",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#main-concepts",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Main concepts",
    "text": "Main concepts\nKubernetes components: A Kubernetes cluster is mainly composed of the following components:\n\nControl Plane components: these make global decisions about the cluster (scheduling pods) as well as detecting and responding to cluster events\n\nThey can be run on any machine in the cluster; however, for simplicity, set up scripts typically start all control plane components on the same machine and do not run user containers there\nSome of the common control plane components are: kube-apiserver, etcd, kube-scheduler, kube-control-manager and cloud-control-manager\n\nNode components: these run on every node, maintaining running pods and providing the Kubernetes runtime environment\n\nSome of the common node components are: kubelet, kube-proxy and container runtime\n\nAddons: they use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features\n\nBecause these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace\nSelected addons are DNS, Web UI (Dashboard), Container Resource Monitoring and Cluster-level Logging\n\n\nAs described above, usually all control plane components are deployed into the same machine or node. Another usual thing to do is to isolate the control plane nodes to prevent them to act as workers; this ensures that their critical mission of overseeing the whole cluster isn’t compromised due to a highly demanding user container.\nWhen dealing with High-Availability, we’ll configure more than one control plane node to ensure redundancy."
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-configuration",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-configuration",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Ansible configuration",
    "text": "Ansible configuration\nFollow the guide’s instructions, clone its git repository and make sure you update the following files. Make a copy of the /inventory/sample directory and name it /inventory/my-cluster. After that, modify the following files according to your servers:\n\n\ninventory/my-cluster/hosts.ini\n\n[master]\nk3s-control-1\nk3s-control-2\nk3s-control-3\n\n[node]\nk3s-worker-1\nk3s-worker-2\n\n\n\n\n\n\n\nSSH keys\n\n\n\nRemember to exchange your SSH public key with the rest of the servers, else Ansible will fail to launch the commands.\n\n\n\n\ninventory/my-cluster/group_vars/all.yml\n\n# this is the user that has ssh access to these machines\n# You have to exchange your public SSH key with them\nansible_user: \"common-user-to-all-servers\"\n\n# apiserver_endpoint is virtual ip-address which will be configured on each master\n# Make sure this IP is within the network range and is reachable\napiserver_endpoint: \"192.168.30.222\"\n\n# k3s_token is required  masters can talk together securely\n# this token should be alpha numeric only\nk3s_token: \"some-SUPER-DEDEUPER-secret-password\"\n\n# metallb ip range for load balancer\n# You might want to avoid matching ranges with your network     \nmetal_lb_ip_range: \"192.168.30.80-192.168.30.90\"\n\n\n\n\n\n\n\nUsing Alpine server for Ansible\n\n\n\nYou may need to launch the following commands:\n# For cluster servers in different subnets\napk add py3-netaddr\n\n# For installing ansible-galaxy requirements\n# Regenerate SSL certificates\n# Guide: https://wiki.alpinelinux.org/wiki/Generating_SSL_certs_with_ACF\n/sbin/setup-acf\napk add acf-openssl"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-playbook-launch",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#ansible-playbook-launch",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Ansible playbook launch",
    "text": "Ansible playbook launch\nNow launch the following command from the repository root:\n\n\nAnsible command\n\n# Install ansible requirements\nansible-galaxy install -r ./collections/requirements.yml\n\n# Launch the installation playbook\nansible-playbook ./site.yml -i ./inventory/my-cluster/hosts.ini\n\nCheck for any errors during the ansible-playbook execution. If everything is OK, proceed to copy the k3s cluster config into the central server:\n\n\nCopying cluster config and testing\n\n# Make sure the '.kube' folder exists in the local server\nmkdir ~/.kube\n\n# Copy the config from one of the master nodes\nscp common-user-to-all-servers@k3s-control-1:~/.kube/config ~/.kube/config\n\n# Check that we can access the cluster\nsudo kubectl get nodes\n\n\n\n\n\n\n\nWorker nodes not showing in the cluster in the first attempt\n\n\n\nCheck what’s wrong with the ansible steps when reaching workers\n\n\nAfter restarting the nodes and launching the playbook several times, now all nodes appear in the k3s cluster:\n\n\nkubectl check of nodes\n\n~/k3s-ansible # kubectl get nodes\nNAME            STATUS   ROLES                       AGE     VERSION\nk3s-control-1   Ready    control-plane,etcd,master   22m     v1.24.10+k3s1\nk3s-control-2   Ready    control-plane,etcd,master   22m     v1.24.10+k3s1\nk3s-control-3   Ready    control-plane,etcd,master   22m     v1.24.10+k3s1\nk3s-worker-1    Ready    <none>                      5m14s   v1.24.10+k3s1\nk3s-worker-2    Ready    <none>                      5m2s    v1.24.10+k3s1"
  },
  {
    "objectID": "homelab-project/kubernetes-cluster/00-getting-started.html#deployment-example",
    "href": "homelab-project/kubernetes-cluster/00-getting-started.html#deployment-example",
    "title": "00 - Kubernetes - Getting Started",
    "section": "Deployment example",
    "text": "Deployment example\nThe git repository has a deployment example of nginx web servers configured to 3 replicas as well as a load balancer service for it.\nOnce deployed with the following commands:\n\n\nDeploying our first application\n\nkubectl apply -f example/deployment.yml\nkubectl apply -f example/service.yml\n\nWe should see 3 nginx pods and a service:\n\n\nPods deployed on different nodes\n\n~/k3s-ansible # kubectl get pods -o wide\nNAME                     READY   STATUS    RESTARTS   AGE     IP          NODE           NOMINATED NODE   READINESS GATES\nnginx-6fb79bc456-79v99   1/1     Running   0          5m47s   10.42.4.3   k3s-worker-2   <none>           <none>\nnginx-6fb79bc456-gj6zh   1/1     Running   0          5m47s   10.42.3.3   k3s-worker-1   <none>           <none>\nnginx-6fb79bc456-qj472   1/1     Running   0          5m47s   10.42.4.2   k3s-worker-2   <none>           <none>\n\n\n\nLoad-balancer service for the Nginx replicas\n\n~/k3s-ansible # kubectl get service -o wide\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE     SELECTOR\nkubernetes   ClusterIP      10.43.0.1      <none>        443/TCP        34m     <none>\nnginx        LoadBalancer   10.43.170.67   10.0.0.11     80:30466/TCP   6m16s   app=nginx\n\n~/k3s-ansible # kubectl describe service nginx\nName:                     nginx\nNamespace:                default\nLabels:                   <none>\nAnnotations:              <none>\nSelector:                 app=nginx\nType:                     LoadBalancer # Here we see that the service type is a load balancer between all the available nginx replicas\nIP Family Policy:         PreferDualStack\nIP Families:              IPv4\nIP:                       10.43.170.67 # This is the Cluster IP assigned to the service\nIPs:                      10.43.170.67\nLoadBalancer Ingress:     10.0.0.11 # This is the IP assigned by the MetalLB controller as an Ingress access point\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  30466/TCP\nEndpoints:                10.42.3.3:80,10.42.4.2:80,10.42.4.3:80 # Here we can see the endpoints of the nginx replicas\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason        Age    From                Message\n  ----    ------        ----   ----                -------\n  Normal  IPAllocated   6m34s  metallb-controller  Assigned IP [\"10.0.0.11\"]\n  Normal  nodeAssigned  6m34s  metallb-speaker     announcing from node \"k3s-control-2\" with protocol \"layer2\""
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html",
    "href": "homelab-project/requirements/00-requirements.html",
    "title": "Homelab Requirements",
    "section": "",
    "text": "Any relatively modern desktop computes can be used as a homelab. In my case I’ll be using an HP Compaq Elite 8300 with an Intel Core i5 (4 cores), 24Gb or RAM and a single NIC."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#virtualization-suite",
    "href": "homelab-project/requirements/00-requirements.html#virtualization-suite",
    "title": "Homelab Requirements",
    "section": "Virtualization suite",
    "text": "Virtualization suite\nIn order to comply with the previous statement, we’ll be using Proxmox VE, a Virtualization suite that acts as a HyperVisor as well as a Linux Containers deployer."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#network-management",
    "href": "homelab-project/requirements/00-requirements.html#network-management",
    "title": "Homelab Requirements",
    "section": "Network management",
    "text": "Network management\nVirtual hosts should have their own internal network so they don’t interfere with other local DHCP servers or even access external networks at all."
  },
  {
    "objectID": "homelab-project/requirements/00-requirements.html#self-hosted-software",
    "href": "homelab-project/requirements/00-requirements.html#self-hosted-software",
    "title": "Homelab Requirements",
    "section": "Self-hosted software",
    "text": "Self-hosted software\nWe’d like to host our own services, and again the best approach should be to deploy containers, either as-is or in an orchestrated environment such as Kubernetes."
  },
  {
    "objectID": "homelab-project/self-hosted-software/06-openproject.html",
    "href": "homelab-project/self-hosted-software/06-openproject.html",
    "title": "06 - Productivity tools - OpenProject",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy OpenProject, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "",
    "text": "Docker"
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#docker-and-docker-compose",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#docker-and-docker-compose",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "Docker and Docker compose",
    "text": "Docker and Docker compose\n\nI’m going to follow Technotim’s guide as my server OS is Ubuntu.\nThere’s really nothing much to say here, just follow the instructions, make sure that you have Internet access to install the packages and check that your user can execute docker commands, like docker version, and docker compose.\nAlso, remember to add your user to the docker group so you don’t need privileged permissions to operate with it:\nsudo usermod -aG docker $USER\nOnce done, log out and log in again to your server to apply the changes on the user groups."
  },
  {
    "objectID": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#overlay-network",
    "href": "homelab-project/self-hosted-software/00-docker-and-docker-compose.html#overlay-network",
    "title": "00 - Containers - Docker & Docker Compose",
    "section": "Overlay network",
    "text": "Overlay network\nAn overlay network is a type of virtual network in Docker that allows multiple Docker hosts to communicate between them. This is pretty handy if, for example, you need some services deployed with Docker in a host to access an authentication service that is hosted on a different Docker host.\n\n\n\n\n\n\nAttention!\n\n\n\nIn order to use this, do we need to install docker swarm?\n\n\nYou can find the official Docker docs regarding overlay networking here."
  },
  {
    "objectID": "homelab-project/self-hosted-software/04-openldap.html",
    "href": "homelab-project/self-hosted-software/04-openldap.html",
    "title": "04 - Authentication - OpenLDAP",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy SonarQube, both on internal network and accessible from outside with HTTPS partially enabled\n\n\n\n\nAn open-source LDAP server\n\nWeb page: Link\nSource code: GitLab and GitHub mirror\nDocker image: Docker Hub - Non-privileged image\n\n\n\n\nAn open-source LDAP administration service with a web interface\n\nWeb page: Link\nSource code: GitHub and the repository which the Docker image that we use is based\nDocker image: Docker Hub\n\n\n\n\nAn open-source web interface that allows LDAP users to change their own passwords.\n\nWeb page: Link\nSource code: GitHub\nDocker image: Docker Hub\n\nThe docker container needs a config file named config.inc.local.php that overwrites the default values defined in config.inc.php; this file is provided in the docker-compose.yml file as a volume.\nThere are a lot of parameters that can be configured, but for the sake of simplicity only LDAP server parameters and password policies are defined here:\n\n\nconfig.inc.local.php\n\n<?php\n// A random keyphrase is required to initialize the service\n$keyphrase = getenv('SSP_KEYPHRASE');\n$debug = false;\n\n// LDAP server config\n$ldap_url = \"ldap://openldap:1389\";\n$ldap_starttls = false;\n$ldap_binddn = \"cn=admin,dc=protossnet,dc=local\";\n$ldap_bindpw = getenv('LDAP_BIND_PASSWORD');\n\n$ldap_base = \"ou=users,dc=protossnet,dc=local\";\n$ldap_login_attribute = \"uid\";\n$ldap_fullname_attribute = \"displayName\";\n$ldap_filter = \"(&(objectClass=inetOrgPerson)($ldap_login_attribute={login}))\";\n\n// Password policies that new passwords must comply\n$pwd_min_length = 8;\n$pwd_max_length = 16;\n$pwd_min_lower = 1;\n$pwd_min_upper = 1;\n$pwd_min_digit = 1;\n$pwd_min_special = 1;\n$pwd_special_chars = \"^@%!-_\";\n$pwd_no_reuse = true;\n$pwd_diff_login = true;\n$pwd_diff_last_min_chars = 0;\n$pwd_no_special_at_ends = false;\n\n// \"manager\" means that the user defined in ldap_binddn is the responsible of\n// changing the user's passwords within LDAP (the rest of users don't have enough privileges)\n$who_change_password = \"manager\";\n$lang = \"en\";\n$allowed_lang = array(\"en\");\n?>"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#install-docker-and-docker-compose",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#install-docker-and-docker-compose",
    "title": "08 - Remote access - Guacamole",
    "section": "Install Docker and Docker Compose",
    "text": "Install Docker and Docker Compose\nWe’ll be using a dedicated Alpine LXC to host Guacamole. Start by installing Docker as explained in this post. The difference here is that the package manager is apk instead of apt, so the commands are:\n\n\nInstalling Docker and Docker Compose in Alpine\n\napk update\napk add docker docker-cli docker-compose docker-cli-compose\n# Make docker start on reboot\nrc-update add docker default"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#install-additional-dependencies",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#install-additional-dependencies",
    "title": "08 - Remote access - Guacamole",
    "section": "Install additional dependencies",
    "text": "Install additional dependencies\nApart from Docker and Docker Compose, we need the following dependencies: - Git to clone boschkundendiest repository - OpenSSL to generate self-signed certificates - Vim to edit some files\n\n\nInstalling OpenSSL in Alpine\n\napk add git openssl vim"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#clone-repository",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#clone-repository",
    "title": "08 - Remote access - Guacamole",
    "section": "Clone repository",
    "text": "Clone repository\n\n\nClone Guacamole Docker Compose repo\n\ngit clone https://github.com/boschkundendienst/guacamole-docker-compose.git"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#modify-the-docker-compose.yml-file",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#modify-the-docker-compose.yml-file",
    "title": "08 - Remote access - Guacamole",
    "section": "Modify the docker-compose.yml file",
    "text": "Modify the docker-compose.yml file\nBy default, this Docker Compose file does the following:\n\nCreates a network guacnetwork_compose with the bridge driver\nCreates a service guacd_compose from guacamole/guacd connected to guacnetwork_compose\nCreates a service postgres_guacamole_compose from postgres connected to guacnetwork_compose\nCreates a service nginx_guacamole_compose from nginx connected to guacnetwork_compose\n\nWe need to modify two things:\n\nFirst of all, replace variables POSTGRES_USER and POSTGRES_PASSWORD with environment variables that be stored in the secured .env file at the same level as the docker-compose.yml file:\n\n\n\nDocker compose env variables\n\nservices:\n    # [...]\n    postgres:\n        # [...]\n        environment:\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n            POSTGRES_USER: ${POSTGRES_PASSWORD}\n    # [...]\n    guacamole:\n        # [...]\n        environment:\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n            POSTGRES_USER: ${POSTGRES_PASSWORD}\n\n\n\n.env file\n\nPOSTGRES_PASSWORD=\"Your-super-secure-password\"\nPOSTGRES_USER=\"Your-postgres-username\"\n\n\nSecond, as we’ll be using Cloudflare Zero-Trust tunnels for secure connections, we won’t be needing the nginx service, so we need to correctly map the Guacamole service port to a valid host port and disable all nginx configuration:\n\n\n\nDocker compose nginx-related changes\n\nservices:\n    guacamole:\n        # [...]\n        ports:\n        ## enable next line if not using nginx\n        - 8080:8080/tcp # Guacamole is on \"localhost:8080/guacamole\", not \"localhost:8080/\".\n        ## enable next line when using nginx\n        # - 8080/tcp\n# [...]\n# Comment anything nginx-related, we don't need it\n########### optional ##############\n#  # nginx\n#  nginx:\n#   container_name: nginx_guacamole_compose\n#   restart: always\n#   image: nginx\n#   volumes:\n#   - ./nginx/templates:/etc/nginx/templates:ro\n#   - ./nginx/ssl/self.cert:/etc/nginx/ssl/self.cert:ro\n#   - ./nginx/ssl/self-ssl.key:/etc/nginx/ssl/self-ssl.key:ro\n#   ports:\n#   - 8443:443\n#   links:\n#   - guacamole\n#   networks:\n#     guacnetwork_compose:\n####################################################################################"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#launch-the-preparation-script-from-the-repository",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#launch-the-preparation-script-from-the-repository",
    "title": "08 - Remote access - Guacamole",
    "section": "Launch the preparation script from the repository",
    "text": "Launch the preparation script from the repository\nAt the root of the repository, make sure that ./prepare.sh has execution permissions and launch it as root or sudo user if you’re using another distribution with an non-privileged user:\n\n\nPreparing the environment for Guacamole\n\nchmod u+x prepare.sh\n./prepare.sh\n\nThis will create the required paths for the docker volumes, initialize the PostgreSQL database, and SSL certificates for nginx (even though we won’t be using them).\nOnce done, we can deploy the docker-compose.yml file:\n\n\nDeploying Guacamole\n\ndocker compose up -d\n\nIf everything is OK, guacamole web portal shall be available on <guacamole-server>:8080/guacamole:\n\n\n\nGuacamole login\n\n\n\n\n\n\n\n\nChange the default credentials!\n\n\n\nDefault username and password are guacadmin both. Make sure to login and change the password first thing!\nAlternative, it’s even more secure to create another user with all permissions and delete the guacadmin one"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#create-groups-of-connections",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#create-groups-of-connections",
    "title": "08 - Remote access - Guacamole",
    "section": "Create groups of connections",
    "text": "Create groups of connections\n\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#create-ssh-connections",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#create-ssh-connections",
    "title": "08 - Remote access - Guacamole",
    "section": "Create SSH connections",
    "text": "Create SSH connections\nGo to the admin panel, select Connections and create a new one. A simple SSH connection only needs these parameters:\n\nName: the connection name. It shall describe the remote server\nLocation: select a group for this connection\nProtocol: select SSH\n\nOn Parameters, configure the following fields:\n\nNetwork:\n\nHost name: either the IP or the FQDN of the server\nPort: 22 by default, change it if it is different\n\nAuthentication:\n\nUser name: the user name to connect as\nPassword: the password, if password authentication is enabled in the remote server\nPrivate key: the openssl private key of the shared public key with the server. It must be\n\n\nSave the connection and go back to the main menu. The connection should be available from there."
  },
  {
    "objectID": "homelab-project/self-hosted-software/08-guacamole.html#create-remote-desktop-connections",
    "href": "homelab-project/self-hosted-software/08-guacamole.html#create-remote-desktop-connections",
    "title": "08 - Remote access - Guacamole",
    "section": "Create Remote Desktop connections",
    "text": "Create Remote Desktop connections\n\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "homelab-project/self-hosted-software/07-sonarqube.html",
    "href": "homelab-project/self-hosted-software/07-sonarqube.html",
    "title": "07 - Code analysis - SonarQube",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy SonarQube, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/self-hosted-software/07-sonarqube.html#prerequisites",
    "href": "homelab-project/self-hosted-software/07-sonarqube.html#prerequisites",
    "title": "07 - Code analysis - SonarQube",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCreating a GitHub app\nFollow the official GitHub App creation guide to create a new app. The most important steps to be aware of are:\n\nThe App name: short and concise, indicating its purpose. In our case we’ll define it as SonarQube App\nThe description: A brief description regarding the app nature\nHomepage URL: mandatory, but for SonarQube we can use any URL. We’ll put the URL of our SonarQube instance\nUser authorization callback URL: it’s our instance’s base URL, so in this case it would be the same as our Homepage URL\nWebhook URL: SonarQube guide recommends disabling this feature. We’ll explore this option in the future\nPermissions: configure the permissions as told in the SonarQube guide\n\nClick on Create App and you’re finished\n\n\nInstalling the GitHub app in your organization\nFollow the official GitHub App installation guide to install the newly created App into our organization (i.e. the server where SonarQube is running).\n\nBasically, browse to your GitHub Apps setting page and select your app\nIn the left sidebar, click Install App and select your account\nSelect whether you want the app to access all of your repositories or only some of them\n\nNow your up is alive and running for your account\n\n\nUpdating the SonarQube global settings with the GitHub App information\nGo back to your SonarQube account and follow the steps from the SonarQube guide:\nNavigate to Administration > Configuration > General Settings > DevOps Platform Integrations > GitHub and specify the following settings:\n\nConfiguration name: mandatory but only used in Enterprise and Data Center Edition. Give it a concise name, for example GitHub personal projects\nGitHub API URL: it’s always the same. As we are using GitHub.com, its URL is https://api.github.com/\nGitHub App ID: found on your GitHub App’s page on GitHub at Settings > Developer Settings > GitHub Apps\nClient ID: same as with GitHub App ID, this info is found at the same place\nClient Secret: this is generated on the GitHub App page, by clicking the Generate a new client secret button. Keep it safe as it won’t appear again! You can also encrypt it instead of saving it as plain text by following this guide.\nPrivate Key: it is the GitHub App’s private key, in a .pem form factor. Generate a private key by clicking Generate a private key in the App’s setting page, you have to save it in your machine. Then, copy and paste its content in this field. It can also be encrypted like the Client Secret.\nWebhook Secret: as we’ve disabled Webhooks, leave this field empty\n\nOnce done, accept the settings. The integration should appear right after that, and if everything is correct, a green check should appear following the Configuration valid text:\n\n\n\nGitHub integration in SonarQube\n\n\n\n\n\n\n\n\nTODO: implement secret key encryption in SonarQube via docker-compose\n\n\n\nInvestigate how to include the secretKey file as a secret and overwrite its location in sonar.properties config."
  },
  {
    "objectID": "homelab-project/self-hosted-software/07-sonarqube.html#analyzing-projects-with-github-actions",
    "href": "homelab-project/self-hosted-software/07-sonarqube.html#analyzing-projects-with-github-actions",
    "title": "07 - Code analysis - SonarQube",
    "section": "Analyzing projects with GitHub Actions",
    "text": "Analyzing projects with GitHub Actions\nNow that we have the GitHub App attached to our SonarQube instance, we can configure a GitHub Action to analyse our code on a trigger basis.\n\nCreate GitHub secrets\nYou can create repository secrets from your GitHub repository (more info here). Basically, we need to create two secrets that will contains SonarQube tokens (follow this guide to generate the required tokens):\n\nSonar Token: Generate a SonarQube token and, in GitHub, create a new repository secret in GitHub with SONAR_TOKEN as the name and the generated token as the Value\nSonar Host URL: In GitHub, create a new repository secret with SONAR_HOST_URL as the Name and your SonarQube server URL as the Value\n\n\n\nConfiguring your .github/workflows/build.yml file\nThe best way to prepare the Workflow .yml is to follow the steps that SonarQube gives you when creating a new project. Also, for GitHub Action to work there must be a project already created in SonarQube:\n\nLogin to your SonarQube instance and click on Create project. Then select GitHub:\n\n\n\nSonarQube - Create Project button\n\n\nSelect the organization and repository to analyse:\n\n\n\nSonarQube - Select Repository\n\n\nSelect the GitHub actions integration:\n\n\n\nSonarQube - Select CI Integration\n\n\nNext, follow the steps to store some required secrets for GitHub to remotely connect to your SonarQube instance:\n\n\n\nSonarQube - Create GitHub secrets\n\n\nAfter that, select the build type. In this case I’ll select Maven, and after that a YAML sample is provided:\n\n\n\nSonarQube - Select Build type\n\n\nThe YAML file configures the following steps:\n\nCheckout of the Repository\nJDK setup\nSonarQube packages cache initialization\nMaven packages cache initialization\nBuild and analyze the project\n\n\n\n\n\n\n\n\nReview and fix the YAML file according to your project!\n\n\n\n\nCheck that the JDK version matches your project’s JDK version\nThe SonarQube tutorial expects to execute mvn from the repository root path! When working with monorepositories we need to change this on workflow YML. One way to do it is to define the proper path of the pom.xml file with the -f option:\n\njobs:\n    build:\n    [...]\n    steps:\n        - name: Build and analyze\n          run: mvn verify [...] -f <path-to-pom.xml>"
  },
  {
    "objectID": "homelab-project/self-hosted-software/05-nextcloud.html",
    "href": "homelab-project/self-hosted-software/05-nextcloud.html",
    "title": "05 - Productivity tools - NextCloud",
    "section": "",
    "text": "TODO\n\n\n\nAdd properly explained procedure to deploy NextCloud, both on internal network and accessible from outside with HTTPS partially enabled"
  },
  {
    "objectID": "homelab-project/self-hosted-software/03-dashy.html",
    "href": "homelab-project/self-hosted-software/03-dashy.html",
    "title": "03 - Homelab dashboard - Dashy",
    "section": "",
    "text": "Dashy is a lightweight dashboard that allows users, devops and sysadmins to quickly access to its services:\n\nGithub repository\nDocumentation"
  },
  {
    "objectID": "homelab-project/self-hosted-software/03-dashy.html#installation",
    "href": "homelab-project/self-hosted-software/03-dashy.html#installation",
    "title": "03 - Homelab dashboard - Dashy",
    "section": "Installation",
    "text": "Installation\nFollow this video for a simple Docker deployment and initial configuration:\n\n\nSynology\nThe docker image can also be deployed inside a Synology NAS. Follow the official Dashy’s documentation on how to do it.\n\n\nDocker compose\nWe can also create a docker-compose.yml file for an easier deployment. Follow Dashy’s documentation and modify it according to your own architecture.\nJust add some volumes for both config.yml and the item-icons folder so we don’t lose them when restarting the container.\n\n\ndocker-compose.yml\n\n---\nversion: \"3.8\"\nservices:\n  dashy:\n    # To build from source, replace 'image: lissy93/dashy' with 'build: .'\n    # build: .\n    image: lissy93/dashy\n    container_name: Dashy\n    # Pass in your config file below, by specifying the path on your host machine\n    # volumes:\n      # - /root/my-config.yml:/app/public/conf.yml\n    ports:\n      - 4000:80\n    # Set any environmental variables\n    environment:\n      - NODE_ENV=production\n    # Specify your user ID and group ID. You can find this by running `id -u` and `id -g`\n    #  - UID=1000\n    #  - GID=1000\n    # Specify restart policy\n    restart: unless-stopped\n    # Volumes\n    volumes:\n      - $PWD/public/conf.yml:/app/public/conf.yml\n      - $PWD/public/item-icons:/app/public/item-icons\n    # Configure healthchecks\n    healthcheck:\n      test: ['CMD', 'node', '/app/services/healthcheck']\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n\nThen locally clone the GitHub repository of icons to beautify the panels:\ncd public/item-icons\ngit clone https://github.com/walkxcode/dashboard-icons.git\nAnd here is a basic config.yml with enabled built-in authentication and some services already configured into its proper sections:\n\n\n\n\n\n\nRemember to change the admin hash\n\n\n\nThe hash parameter within auth.users[admin].hash is the password converted to a sha256 hash. You can obtain it by simply invoking the following command from a terminal:\necho -n \"your-admin-super-secret-password\" | sha256sum\nThe hash is the first 64 characters of the output (ignore the empty spaces and dashes at the end).\n\n\n\n\npublic/conf.yml\n\nappConfig:\n  theme: colorful\n  layout: auto\n  iconSize: large\n  language: en\n  auth:\n    enableGuestAccess: true\n    users:\n      - user: admin\n        hash: hash-of-a-password-you-choose-using-sha256-hashing\n        type: admin\ndefaultOpeningMethod: newtab\nwebSearch:\n    disableWebSearch: false\n    searchEngine: duckduckgo\n    openingMethod: newtab\n    searchBangs: {}\n  enableFontAwesome: true\n  enableMaterialDesignIcons: true\n  allowConfigEdit: true\npageInfo:\n  title: Home Lab\n  description: Welcome to your Home Lab!\n  navLinks:\n    - title: GitHub\n      path: https://github.com/ProtossGP32/DevOps-training-projects\n    - title: Introduction to HomeLab\n      path: https://protossgp32.github.io/DevOps-training-projects/homelab-project/00-introduction.html\n  footerText: ''\nsections:\n  - name: Server administration\n    icon: fas fa-server\n    items:\n      - title: Proxmox\n        description: Virtualisation manager\n        url: https://pve.protossnet.local\n        icon: dashboard-icons/png/proxmox.png\n        statusCheck: true\n      - title: phpLDAPadmin\n        description: LDAP administrator\n        url: http://ldap-server.protossnet.local:8080\n        icon: dashboard-icons/png/phpldapadmin.png\n        statusCheck: true\n    displayData:\n      sortBy: default\n      rows: 1\n      cols: 1\n      collapsed: false\n      hideForGuests: true\n\n  - name: Productivity Tools\n    icon: fas fa-regular fa-window\n    items:\n      - title: NextCloud\n        description: Cloud office suite and collaboration platform\n        url: http://sonic.protossnet.local:8080\n        icon: dashboard-icons/png/nextcloud.png\n        statusCheck: true\n      - title: OpenProject\n        description: Projects management suite\n        url: http://sonic.protossnet.local:8081\n        icon: dashboard-icons/png/openproject.png\n        statusCheck: true\n      - title: SonarQube\n        description: Static code analysis suite\n        url: http://sonic.protossnet.local:9000\n        icon: dashboard-icons/png/sonarqube.png\n      - title: VS Code Web\n        description: Cloud based VS Code development environment\n        icon: dashboard-icons/png/vs-code.png\n        statusCheck: true\n\n  - name: User management\n    icon: fas fa-light fa-user\n    displayData:\n      sortBy: default\n      rows: 1\n      cols: 1\n      collapsed: false\n      hideForGuests: false\n    items:\n      - title: Self Service Password\n        description: Allows to change your user password\n        icon: dashboard-icons/png/ltb-logo.png\n        url: https://ssp-protoss.cifoweb.dev/\n        statusCheck: true"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#get-a-domain-name",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#get-a-domain-name",
    "title": "01 - External access - Cloudflare",
    "section": "Get a Domain name",
    "text": "Get a Domain name"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#create-a-cloudflare-account",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#create-a-cloudflare-account",
    "title": "01 - External access - Cloudflare",
    "section": "Create a Cloudflare account",
    "text": "Create a Cloudflare account"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#setup-the-domain-name-in-cloudflare",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#setup-the-domain-name-in-cloudflare",
    "title": "01 - External access - Cloudflare",
    "section": "Setup the Domain name in Cloudflare",
    "text": "Setup the Domain name in Cloudflare"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#zero-trust-dashboard",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#zero-trust-dashboard",
    "title": "01 - External access - Cloudflare",
    "section": "Zero Trust Dashboard",
    "text": "Zero Trust Dashboard\n\n\n\n\n\n\nTODO\n\n\n\nAdd some screenshots of the nameserver creation procedure in Zero Trust Dashboard"
  },
  {
    "objectID": "homelab-project/self-hosted-software/01-cloudflare.html#deploying-cloudflared-as-docker-container",
    "href": "homelab-project/self-hosted-software/01-cloudflare.html#deploying-cloudflared-as-docker-container",
    "title": "01 - External access - Cloudflare",
    "section": "Deploying cloudflared as docker container",
    "text": "Deploying cloudflared as docker container\nAdapt the proposed docker compose file in this link to launch it as a standalone container.\nCloudflare recommends to create only a tunnel for each network, so we’ll deploy it on a container within our private network:\n\n\ndocker-compose.yml\n\nversion: '3.2'\nname: cloudflared\nservices:\n  tunnel:\n    #container_name: cloudflared-tunnel\n    image: cloudflare/cloudflared\n    # This sysctl param change doesn't seem to work on Apache OSes\n    sysctls:\n      net.core.rmem_max: 2500000\n    restart: unless-stopped\n    command: tunnel --metrics 0.0.0.0:3333 run\n    environment:\n      # Add your cloudflare token inside a secured '.env' file\n      - TUNNEL_TOKEN=${CLOUDFLARE_TOKEN}\n    # Add autoheal feature to ensure it's restarted on failure\n    labels:\n      - autoheal=true\n    # TODO: Official cloudflared image doesn't have neither curl nor wget nor dig\n    # so we can't launch the healthcheck! We either create a new image that installs\n    # any of the required commands or try to get the health status from another container\n    # or from outside, exposing the port\n    #healthcheck:\n    #  test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:3333/ready\"]\n    #  interval: 10s\n    #  timeout: 3s\n    #  retries: 3\n    #  start_period: 30s\n\n  # Autoheal is a workaround to restart any container which healthcheck fails\n  #autoheal:\n  #  image: willfarrell/autoheal:1.2.0\n  #  volumes:\n  #    - \"/var/run/docker.sock:/var/run/docker.sock\"\n  #  environment:\n  #    AUTOHEAL_CONTAINER_LABEL: autoheal\n\nAdd the CLOUDFLARE_TOKEN value inside a secure .env file in the same dir as the docker-compose.yml file:\nCLOUDFLARE_TOKEN=token-provided-by-cloudflare\n\n\n\n\n\n\nWarning with system limits!\n\n\n\nIf you see a message like this one when launching docker compose logs tunnel:\nfailed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 2048 kiB, got: 416 kiB)\nCheck the following link for a deep explanation: LINK\nWhat we must do is create an init container and change some system params, similar to what we are doing with SonarQube.\nAlso, we could try configuring the sysctl parameters using the docker compose file.\nOn some OS it doesn’t seem to work, such as Alpine, but if Cloudflare shows that the tunnel is connected, then this shouldn’t be a problem."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#prepare-an-entry-point-vm",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#prepare-an-entry-point-vm",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Prepare an entry-point VM",
    "text": "Prepare an entry-point VM\nWe’ll need an entry-point server that acts as both HTTP requests router and SSL provider (Traefik)."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#make-sure-yo-have-a-domain-name",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#make-sure-yo-have-a-domain-name",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Make sure yo have a Domain name",
    "text": "Make sure yo have a Domain name\n\n\n\n\n\n\nUse Cloudflare as DNS provider\n\n\n\nThis will ease the process and secure connections even more"
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-traefik",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-traefik",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Install Traefik",
    "text": "Install Traefik\n\n\n\n\n\n\nI couldn’t make it work!\n\n\n\nI don’t know if it’s because I’m using a Cloudflare Zero-Trust tunnel instead of assigning DNS records pointing towards my public IP, but either Traefik keeps responding with a 404 Page not found error or it fails due to too many redirections.\nI’ll try it again following the official documentation.\nThere’s another guide here that could be useful."
  },
  {
    "objectID": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-portainer",
    "href": "homelab-project/self-hosted-software/02-traefik-and-portainer.html#install-portainer",
    "title": "02 - Routing and SSL certificates - Traefik and Portainer",
    "section": "Install Portainer",
    "text": "Install Portainer\n\nPortainer is a tool that eases the management and deployment of Docker containers in your infrastructure. Its installation is also pretty straightforward:\n\nFor management purposes, we are going to create a docker folder in our home directory. This is where we’ll store all of our Docker services configurations:\n\nmkdir docker\necho $PWD\n# This should return the following\n\n\n\n\n\n\nTODO\n\n\n\nPending to install Portainer and control all Docker hosts"
  },
  {
    "objectID": "homelab-project/00-introduction.html",
    "href": "homelab-project/00-introduction.html",
    "title": "Homelab project introduction",
    "section": "",
    "text": "First of all\nBig thank you to Lars Kiesow and his awesome guide on how to configure a home lab with Proxmox VE from the start and very well explained, no shortcuts at all.\n\nLink to the guide here\nYou can also check the original MD files in its GitHub repository here\nAlso check the proxmox branch for some new insights still not pushed to the main branch here\n\n\n\nRationale\n\n\n\n\n\n\nTODO\n\n\n\nPending. Add some architecture diagram to better understand what we’re trying to achieve."
  }
]